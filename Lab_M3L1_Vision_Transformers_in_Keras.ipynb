{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3057f4b9-bfce-4093-86a6-b8fb9b4702c5",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "  <a href=\"https://cognitiveclass.ai/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0321ENSkillsNetwork951-2022-01-01\">\n",
    "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DL0321EN-SkillsNetwork/image/IDSN-logo.png\" width=\"400\">\n",
    "  </a>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f987e3d7-707a-4548-8f09-c7bd5bbfd619",
   "metadata": {},
   "source": [
    "<h1 align=left><font size = 6>Lab: Vision Transformers Using Keras </font></h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5c1104-b4b0-427a-befb-1b205b14486c",
   "metadata": {},
   "source": [
    "<h5>Estimated time: 90 minutes</h5>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e38e483-8620-422f-a38b-4b5fe1244513",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this notebook, you will learn to build a CNN-Vision Transformer (ViT) hybrid image classification model. You will start by loading an existing CNN that is good at recognizing small patterns in pictures. Then, you'll learn how to improve it with a transformer, which helps the model see and use wider and more complex relationships in an image. The notebook covers important topics like preparing your image data, making your model smarter with both local and global learning, and saving your best results automatically. By the end, you'll understand how CNN-ViT hybrid models work and how to train, evaluate, and visualize them for any image classification task.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7955effd",
   "metadata": {},
   "source": [
    "<h2>Objective</h2>\n",
    "\n",
    "This notebook demonstrates how to use a custom-trained Keras CNN model to extract feature maps and feed them into a ViT architecture.\n",
    "\n",
    "After completing this lab, you will be able to:\n",
    "<ul>\n",
    "    \n",
    "1. Load the custom-trained CNN model\n",
    "2. Extract feature maps from the CNN\n",
    "3. Prepare tokens for the Vision Transformer\n",
    "4. Build the Vision Transformer encoder\n",
    "5. Train and evaluate the combined model\n",
    "\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08864f02",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "\n",
    "<font size = 3> \n",
    "    \n",
    "1. [ Custom positional embedding layer](#Custom-positional-embedding-layer)\n",
    "3. [Transformer block implementation](#Transformer-block-implementation)\n",
    "4. [Hybrid model builder function](#Hybrid-model-builder-function)\n",
    "5. [Model loading and setup](#Model-loading-and-setup)\n",
    "6. [Data generator configuration](#Data-generator-configuration)\n",
    "7. [Model checkpoint setup](#Model-checkpoint-setup)\n",
    "8. [Model training and compilation](#Model-training-and-compilation)\n",
    "9. [Model shape validation](#Model-shape-validation)\n",
    "10. [Training results visualization](#Training-results-visualization)\n",
    "\n",
    "</font> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f18f62d",
   "metadata": {},
   "source": [
    "## Data download and extraction\n",
    "Let's begin by downloading the dataset to evaluate the models.\n",
    "Here, you declare:\n",
    "1. The dataset URL from which the dataset would be downloaded\n",
    "2. The dataset downloading primary function, based on the `skillsnetwork` library\n",
    "3. The dataset fallback downloading function, based on regular `http` downloading functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a9f0820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write permissions available for downloading and extracting the dataset tar file\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11fb020e90e448799c0235b7c0093a08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading images-dataSAT.tar:   0%|          | 0/20243456 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb581ce310494d3481375bb242247a74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6003 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to '.'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import skillsnetwork\n",
    "\n",
    "data_dir = \".\"\n",
    "dataset_url = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/4Z1fwRR295-1O3PMQBH6Dg/images-dataSAT.tar\"\n",
    "\n",
    "\n",
    "def check_skillnetwork_extraction(extract_dir):\n",
    "    \"\"\"Check if the environment allows symlink creation for download/extraction.\"\"\"\n",
    "    symlink_test = os.path.join(extract_dir, \"symlink_test\")\n",
    "    if not os.path.exists(symlink_test):\n",
    "        os.symlink(os.path.join(os.sep, \"tmp\"), symlink_test)\n",
    "        print(\"Write permissions available for downloading and extracting the dataset tar file\")\n",
    "        os.unlink(symlink_test)\n",
    "\n",
    "async def download_tar_dataset(url, tar_path, extract_dir):\n",
    "    \"\"\"Download and extract dataset tar file asynchronously.\"\"\"\n",
    "    if not os.path.exists(tar_path):\n",
    "        try:\n",
    "            print(f\"Downloading from {url}...\")\n",
    "            import httpx\n",
    "            async with httpx.AsyncClient() as client:\n",
    "                response = await client.get(url, follow_redirects=True)\n",
    "                response.raise_for_status()\n",
    "                with open(tar_path, \"wb\") as f:\n",
    "                    f.write(response.content)\n",
    "            print(f\"Successfully downloaded '{tar_path}'.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Download error: {e}\")\n",
    "    else:\n",
    "        print(f\"Dataset tar file already exists at: {tar_path}\")\n",
    "    import tarfile\n",
    "    with tarfile.open(tar_path, 'r:*') as tar_ref:\n",
    "        tar_ref.extractall(path=extract_dir)\n",
    "        print(f\"Successfully extracted to '{extract_dir}'.\")\n",
    "\n",
    "try:\n",
    "    check_skillnetwork_extraction(data_dir)\n",
    "    await skillsnetwork.prepare(url=dataset_url, path=data_dir, overwrite=True)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    print(\"Primary download/extraction method failed.\")\n",
    "    print(\"Falling back to manual download and extraction...\")\n",
    "    import tarfile\n",
    "    import httpx\n",
    "    from pathlib import Path\n",
    "    file_name = Path(dataset_url).name\n",
    "    tar_path = os.path.join(data_dir, file_name)\n",
    "    await download_tar_dataset(dataset_url, tar_path, data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c1f18c",
   "metadata": {},
   "source": [
    "## Package installation\n",
    "\n",
    "Install the required basic Python packages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c7ca672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 52.6 ms, sys: 35.2 ms, total: 87.8 ms\n",
      "Wall time: 17.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%capture captured_output\n",
    "%pip install numpy==1.26\n",
    "%pip install matplotlib==3.9.2\n",
    "%pip install skillsnetwork"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd033f37-165f-41b8-90e2-a2439f035575",
   "metadata": {},
   "source": [
    "### Install Tensorflow library for Keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d66b191c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow==2.19 in /opt/conda/lib/python3.12/site-packages (2.19.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.19) (2.3.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.19) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.19) (25.9.23)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.19) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.19) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.19) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.19) (3.4.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.19) (24.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.19) (5.29.5)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.19) (2.32.3)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.19) (75.8.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.19) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.19) (3.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.19) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.19) (2.0.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.19) (1.76.0)\n",
      "Requirement already satisfied: tensorboard~=2.19.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.19) (2.19.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.19) (3.12.0)\n",
      "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.19) (1.26.0)\n",
      "Requirement already satisfied: h5py>=3.11.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.19) (3.15.1)\n",
      "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.19) (0.5.4)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.12/site-packages (from astunparse>=1.6.0->tensorflow==2.19) (0.45.1)\n",
      "Requirement already satisfied: rich in /opt/conda/lib/python3.12/site-packages (from keras>=3.5.0->tensorflow==2.19) (14.2.0)\n",
      "Requirement already satisfied: namex in /opt/conda/lib/python3.12/site-packages (from keras>=3.5.0->tensorflow==2.19) (0.1.0)\n",
      "Requirement already satisfied: optree in /opt/conda/lib/python3.12/site-packages (from keras>=3.5.0->tensorflow==2.19) (0.18.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow==2.19) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow==2.19) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow==2.19) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow==2.19) (2024.12.14)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.12/site-packages (from tensorboard~=2.19.0->tensorflow==2.19) (3.10)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.12/site-packages (from tensorboard~=2.19.0->tensorflow==2.19) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.12/site-packages (from tensorboard~=2.19.0->tensorflow==2.19) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow==2.19) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.12/site-packages (from rich->keras>=3.5.0->tensorflow==2.19) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.12/site-packages (from rich->keras>=3.5.0->tensorflow==2.19) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow==2.19) (0.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "CPU times: user 101 ms, sys: 47.7 ms, total: 149 ms\n",
      "Wall time: 7.18 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%pip install tensorflow==2.19"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e461a6d4-57ea-4431-bdc9-c30e436999f0",
   "metadata": {},
   "source": [
    "### Install SkLearn ML library for evaluation metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03c5eb16-6bee-4ca9-bb01-271f0f708743",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn==1.7.0 in /opt/conda/lib/python3.12/site-packages (1.7.0)\n",
      "Requirement already satisfied: numpy>=1.22.0 in /opt/conda/lib/python3.12/site-packages (from scikit-learn==1.7.0) (1.26.0)\n",
      "Requirement already satisfied: scipy>=1.8.0 in /opt/conda/lib/python3.12/site-packages (from scikit-learn==1.7.0) (1.16.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/conda/lib/python3.12/site-packages (from scikit-learn==1.7.0) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/conda/lib/python3.12/site-packages (from scikit-learn==1.7.0) (3.6.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "CPU times: user 92.4 ms, sys: 29.2 ms, total: 122 ms\n",
      "Wall time: 6.66 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%pip install scikit-learn==1.7.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134c4ade",
   "metadata": {},
   "source": [
    "## Library imports and setup\n",
    "\n",
    "Import essential libraries for data manipulation and visualization, and suppress warnings for cleaner notebook output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd0fcdf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 582 ms, sys: 186 ms, total: 769 ms\n",
      "Wall time: 2.68 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "import time\n",
    "import httpx\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "def present_time():\n",
    "        return datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a0a554",
   "metadata": {},
   "source": [
    "### TensorFlow/Keras library imports\n",
    "\n",
    "Sets environment variables to reduce TensorFlow logging noise and imports Keras modules for model building and training. Detects GPU availability for device assignment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99804321",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1764005001.852899    2580 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1764005001.873733    2580 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1764005001.953230    2580 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1764005001.953270    2580 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1764005001.953272    2580 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1764005001.953274    2580 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device available for training: cpu\n",
      "CPU times: user 3.95 s, sys: 782 ms, total: 4.73 s\n",
      "Wall time: 20.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout, BatchNormalization\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.initializers import HeUniform\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "gpu_list = tf.config.list_physical_devices('GPU')\n",
    "device = \"gpu\" if gpu_list != [] else \"cpu\"\n",
    "print(f\"Device available for training: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc53fe7",
   "metadata": {},
   "source": [
    "## Model download helper\n",
    "\n",
    "Now, define an asynchronous function to download model files from given URLs, if they are not already present locally. \n",
    "You use `httpx` for asynchronous HTTP requests with error handling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e211b54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def download_model(url, model_path):\n",
    "    if not os.path.exists(model_path):\n",
    "        try:\n",
    "            print(f\"Downloading from {url}...\")\n",
    "            import httpx\n",
    "            async with httpx.AsyncClient() as client:\n",
    "                response = await client.get(url, follow_redirects=True)\n",
    "                response.raise_for_status()\n",
    "                with open(model_path, \"wb\") as f:\n",
    "                    f.write(response.content)\n",
    "            print(f\"Successfully downloaded '{model_path}'.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Download error: {e}\")\n",
    "    else:\n",
    "        print(f\"Model file already downloaded at: {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e570be57-93e3-49cd-9832-4979909cbeab",
   "metadata": {},
   "source": [
    "## Lab layout\n",
    "1. You will start by loading a pre-trained Convolutional Neural Network (CNN) to act as a robust feature extractor for your image dataset.\n",
    "2. After loading your CNN, you’ll select an intermediate feature map and reshape it into a sequence of tokens, getting your data ready for transformer-based learning.\n",
    "3. You’ll add custom positional embeddings to your tokens so that the model can retain the original spatial structure of your images, even after the features have been flattened.\n",
    "4. Next, you implement a Vision Transformer (ViT) encoder by stacking several transformer blocks, allowing your model to learn global relationships and context throughout the image.\n",
    "5. You’ll combine the CNN and ViT encoder into a single, hybrid model so that you can leverage both the local feature extraction power of CNNs and the global attention mechanism of transformers.\n",
    "6. When preparing your dataset, you’ll use Keras’s ImageDataGenerator to handle data augmentation and to properly encode your labels for multi-class image classification.\n",
    "7. You’ll set up a model checkpoint callback, letting your model automatically save its best weights whenever validation accuracy improves during training, so you always keep the most effective model.\n",
    "8. To ensure everything works smoothly, you’ll check the input and output shapes, which helps you catch architectural mistakes early.\n",
    "9. As your model trains, you’ll visualize both training and validation accuracy and loss, which will help you monitor performance and spot signs of overfitting or underfitting.\n",
    "10. Throughout the process, you can follow clear explanations in the notebook, making it easy for you to understand how each component—from CNN to transformer—is integrated to achieve stronger image classification.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e94bd9",
   "metadata": {},
   "source": [
    "## Model paths and download\n",
    "\n",
    "In the cell below, you define the file paths and URLs for the Keras and PyTorch models and download them using the `download_model` function defined above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be04a5fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model file already downloaded at: ./ai-capstone-keras-best-model-model_downloaded.keras\n"
     ]
    }
   ],
   "source": [
    "data_dir = \".\"\n",
    "\n",
    "keras_model_url = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/U-uPeyCyOQYh0GrZPGsqoQ/ai-capstone-keras-best-model-model.keras\"\n",
    "keras_model_name = \"ai-capstone-keras-best-model-model_downloaded.keras\"\n",
    "keras_model_path = os.path.join(data_dir, keras_model_name)\n",
    "\n",
    "await download_model(keras_model_url, keras_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c801ed-dedc-4148-9c1f-6dd432d91093",
   "metadata": {},
   "source": [
    "### Reproducibility with random seeds\n",
    "\n",
    "Here we fix the random seeds for `random` module, NumPy, and TensorFlow. By initializing these seeds with a constant value (for example, 42), any operations that involve randomness (such as weight initialization, data shuffling, or data augmentation) will produce the exact same sequence of random numbers every time the code is run. This is crucial for ensuring the reproducibility of experimental results and when comparing different models or hyperparameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c483241b-9792-44e9-967f-2df9a6ff2b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed for reproducibility\n",
    "seed_value = 7331\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2b3d89",
   "metadata": {},
   "source": [
    "## Model loading and setup\n",
    "\n",
    "Here, you will load a pre-trained CNN model and learn to work with saved Keras models and prepare them for use in the hybrid architecture.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124a12a8-aafb-48ec-bd8a-bb90073fd971",
   "metadata": {},
   "source": [
    "## Task 1: Load the pre-trained CNN model in `cnn_model` variable using `load_model()` function and print model summary using `summary()` method\n",
    "\n",
    "The `load_model()` function loads the complete Keras model, including architecture, weights, and compilation state. The loaded model serves as the CNN backbone for feature extraction in the hybrid architecture. The `cnn_model.summary()` line can be uncommented to inspect the model architecture and identify appropriate layers for feature extraction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "34f9f5e1-b775-45e1-8bf7-ee0856b5a4c8",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] Unable to synchronously open file (unable to open file: name = 'path/to/your/pretrained_cnn_model.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_model\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Load the pre-trained CNN model\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m cnn_model \u001b[38;5;241m=\u001b[39m \u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpath/to/your/pretrained_cnn_model.h5\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Print the model summary to inspect architecture and layers\u001b[39;00m\n\u001b[1;32m      7\u001b[0m cnn_model\u001b[38;5;241m.\u001b[39msummary()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/keras/src/saving/saving_api.py:196\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m saving_lib\u001b[38;5;241m.\u001b[39mload_model(\n\u001b[1;32m    190\u001b[0m         filepath,\n\u001b[1;32m    191\u001b[0m         custom_objects\u001b[38;5;241m=\u001b[39mcustom_objects,\n\u001b[1;32m    192\u001b[0m         \u001b[38;5;28mcompile\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcompile\u001b[39m,\n\u001b[1;32m    193\u001b[0m         safe_mode\u001b[38;5;241m=\u001b[39msafe_mode,\n\u001b[1;32m    194\u001b[0m     )\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(filepath)\u001b[38;5;241m.\u001b[39mendswith((\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.hdf5\u001b[39m\u001b[38;5;124m\"\u001b[39m)):\n\u001b[0;32m--> 196\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlegacy_h5_format\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model_from_hdf5\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mcompile\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mcompile\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m        \u001b[49m\u001b[43msafe_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msafe_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(filepath)\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.keras\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    204\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile not found: filepath=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    205\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease ensure the file is an accessible `.keras` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    206\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzip file.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    207\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/keras/src/legacy/saving/legacy_h5_format.py:118\u001b[0m, in \u001b[0;36mload_model_from_hdf5\u001b[0;34m(filepath, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[1;32m    116\u001b[0m opened_new_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(filepath, h5py\u001b[38;5;241m.\u001b[39mFile)\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m opened_new_file:\n\u001b[0;32m--> 118\u001b[0m     f \u001b[38;5;241m=\u001b[39m \u001b[43mh5py\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    120\u001b[0m     f \u001b[38;5;241m=\u001b[39m filepath\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/h5py/_hl/files.py:566\u001b[0m, in \u001b[0;36mFile.__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, fs_strategy, fs_persist, fs_threshold, fs_page_size, page_buf_size, min_meta_keep, min_raw_keep, locking, alignment_threshold, alignment_interval, meta_block_size, track_times, **kwds)\u001b[0m\n\u001b[1;32m    557\u001b[0m     fapl \u001b[38;5;241m=\u001b[39m make_fapl(driver, libver, rdcc_nslots, rdcc_nbytes, rdcc_w0,\n\u001b[1;32m    558\u001b[0m                      locking, page_buf_size, min_meta_keep, min_raw_keep,\n\u001b[1;32m    559\u001b[0m                      alignment_threshold\u001b[38;5;241m=\u001b[39malignment_threshold,\n\u001b[1;32m    560\u001b[0m                      alignment_interval\u001b[38;5;241m=\u001b[39malignment_interval,\n\u001b[1;32m    561\u001b[0m                      meta_block_size\u001b[38;5;241m=\u001b[39mmeta_block_size,\n\u001b[1;32m    562\u001b[0m                      \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    563\u001b[0m     fcpl \u001b[38;5;241m=\u001b[39m make_fcpl(track_order\u001b[38;5;241m=\u001b[39mtrack_order, track_times\u001b[38;5;241m=\u001b[39mtrack_times,\n\u001b[1;32m    564\u001b[0m                      fs_strategy\u001b[38;5;241m=\u001b[39mfs_strategy, fs_persist\u001b[38;5;241m=\u001b[39mfs_persist,\n\u001b[1;32m    565\u001b[0m                      fs_threshold\u001b[38;5;241m=\u001b[39mfs_threshold, fs_page_size\u001b[38;5;241m=\u001b[39mfs_page_size)\n\u001b[0;32m--> 566\u001b[0m     fid \u001b[38;5;241m=\u001b[39m \u001b[43mmake_fid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muserblock_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfapl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfcpl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mswmr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mswmr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    568\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(libver, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_libver \u001b[38;5;241m=\u001b[39m libver\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/h5py/_hl/files.py:241\u001b[0m, in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    239\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m swmr \u001b[38;5;129;01mand\u001b[39;00m swmr_support:\n\u001b[1;32m    240\u001b[0m         flags \u001b[38;5;241m|\u001b[39m\u001b[38;5;241m=\u001b[39m h5f\u001b[38;5;241m.\u001b[39mACC_SWMR_READ\n\u001b[0;32m--> 241\u001b[0m     fid \u001b[38;5;241m=\u001b[39m \u001b[43mh5f\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfapl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfapl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr+\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    243\u001b[0m     fid \u001b[38;5;241m=\u001b[39m h5f\u001b[38;5;241m.\u001b[39mopen(name, h5f\u001b[38;5;241m.\u001b[39mACC_RDWR, fapl\u001b[38;5;241m=\u001b[39mfapl)\n",
      "File \u001b[0;32mh5py/_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/h5f.pyx:104\u001b[0m, in \u001b[0;36mh5py.h5f.open\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] Unable to synchronously open file (unable to open file: name = 'path/to/your/pretrained_cnn_model.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load the pre-trained CNN model\n",
    "cnn_model = load_model('path/to/your/pretrained_cnn_model.h5')\n",
    "\n",
    "# Print the model summary to inspect architecture and layers\n",
    "cnn_model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ccbcff-9fd5-4bb3-bc6a-cf00e3339009",
   "metadata": {},
   "source": [
    "Double-click **here** for the solution.\n",
    "<!--\n",
    "cnn_model = load_model(keras_model_path) # Loading the CNN model\n",
    "\n",
    "cnn_model.summary() # Display model summary\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc551ed-bce9-4b1e-b615-ff5c608df8fa",
   "metadata": {},
   "source": [
    "## Task 2: Based on `model.summary()`, get the name of the layer from the CNN model for feature extraction in the variable `feature_layer_name`\n",
    "\n",
    "This is the last convolutional layer, usually before `GlobalAveragePooling2D`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "88a6ab36-c62a-4f2e-906f-761282cd788d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cnn_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 8\u001b[0m\n\u001b[1;32m      4\u001b[0m feature_layer_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_normalization_5\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Criar modelo para extração de features\u001b[39;00m\n\u001b[1;32m      7\u001b[0m feature_extractor \u001b[38;5;241m=\u001b[39m Model(\n\u001b[0;32m----> 8\u001b[0m     inputs\u001b[38;5;241m=\u001b[39m\u001b[43mcnn_model\u001b[49m\u001b[38;5;241m.\u001b[39minput,\n\u001b[1;32m      9\u001b[0m     outputs\u001b[38;5;241m=\u001b[39mcnn_model\u001b[38;5;241m.\u001b[39mget_layer(feature_layer_name)\u001b[38;5;241m.\u001b[39moutput\n\u001b[1;32m     10\u001b[0m )\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Verificar o resumo do modelo de extração de features\u001b[39;00m\n\u001b[1;32m     13\u001b[0m feature_extractor\u001b[38;5;241m.\u001b[39msummary()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cnn_model' is not defined"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Nome da camada de interesse\n",
    "feature_layer_name = \"batch_normalization_5\"\n",
    "\n",
    "# Criar modelo para extração de features\n",
    "feature_extractor = Model(\n",
    "    inputs=cnn_model.input,\n",
    "    outputs=cnn_model.get_layer(feature_layer_name).output\n",
    ")\n",
    "\n",
    "# Verificar o resumo do modelo de extração de features\n",
    "feature_extractor.summary()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dbb3c91-32d8-4f89-9be1-9a5b3997b3e4",
   "metadata": {},
   "source": [
    "Double-click **here** for the solution.\n",
    "<!--\n",
    "feature_layer_name = \"batch_normalization_5\"\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bbea4d3",
   "metadata": {},
   "source": [
    "## Custom positional embedding layer\n",
    "\n",
    "In this cell, you define a custom Keras layer called `AddPositionEmbedding` that implements positional embeddings for the Vision Transformer architecture. Positional embeddings are crucial in transformer models because they lack inherent spatial awareness, unlike convolutional layers that have built-in spatial inductive biases.\n",
    "\n",
    "- The class inherits from `layers.Layer`, making it a proper Keras custom layer\n",
    "- In the `__init__` method, it creates a trainable weight matrix using `self.add_weight()`\n",
    "- The positional embedding has shape `(1, num_patches, embed_dim)` where the first dimension allows broadcasting across batch sizes\n",
    "- The `initializer=\"random_normal\"` ensures the embeddings start with random values that will be learned during training\n",
    "- The `trainable=True` parameter makes these embeddings learnable parameters\n",
    "\n",
    "\n",
    "This layer is essential for the hybrid CNN-ViT architecture because when CNN feature maps are flattened into tokens, spatial relationships are lost. The positional embeddings restore spatial awareness by providing each token with information about its original spatial location in the feature map. This allows the transformer to understand which tokens are spatially adjacent or distant, enabling it to make spatially aware attention decisions.\n",
    "\n",
    "The `call` method adds the positional embeddings to the input tokens using element-wise addition. This is computationally efficient and follows the standard transformer approach, where positional information is added to preserve the embedding dimension.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bf233bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positional embedding that Keras can track\n",
    "@tf.keras.utils.register_keras_serializable(package=\"Custom\")\n",
    "class AddPositionEmbedding(layers.Layer):\n",
    "    def __init__(self, num_patches, embed_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.num_patches = num_patches\n",
    "        self.embed_dim   = embed_dim\n",
    "        self.pos = self.add_weight(\n",
    "            name=\"pos_embedding\",\n",
    "            shape=(1, num_patches, embed_dim),\n",
    "            initializer=\"random_normal\",\n",
    "            trainable=True)\n",
    "\n",
    "    def call(self, tokens):\n",
    "        return tokens + self.pos\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"num_patches\": self.num_patches,\n",
    "            \"embed_dim\":   self.embed_dim,\n",
    "        })\n",
    "        return {**config}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8388357",
   "metadata": {},
   "source": [
    "## Transformer block implementation\n",
    "\n",
    "In this code cell, you will implement a complete transformer encoder block, the fundamental building block of the Vision Transformer architecture. The `TransformerBlock` class encapsulates the standard transformer encoder architecture with multi-head self-attention and feed-forward neural network components.\n",
    "\n",
    "**Role in hybrid architecture:**\n",
    "In the CNN-ViT hybrid, these transformer blocks process the tokenized CNN feature maps, allowing the model to capture long-range spatial dependencies that CNNs might miss due to their limited receptive fields. The self-attention mechanism enables each spatial location to attend to all other locations, providing global context awareness.\n",
    "\n",
    "**Technical architecture:**\n",
    "- **Multi-head attention (MHA):** Uses `layers.MultiHeadAttention` with a configurable number of heads and key dimension equal to embed_dim\n",
    "- **Layer normalization:** Two `LayerNormalization` layers with epsilon=1e-6 for numerical stability\n",
    "- **MLP block:** A two-layer feed-forward network with GELU activation and dropout for regularization\n",
    "- **Residual connections:** Implements skip connections around both the attention and MLP blocks\n",
    "\n",
    "**Parameters:**\n",
    "- `embed_dim`: The dimensionality of token embeddings (typically matches CNN feature map channels)\n",
    "- `num_heads`: Number of attention heads (default 8, must divide embed_dim evenly)\n",
    "- `mlp_dim`: Hidden dimension of the MLP block (typically 4x embed_dim)\n",
    "- `dropout`: Dropout rate for regularization (default 0.1)\n",
    "\n",
    "**Forward pass logic:**\n",
    "Forward pass allows the model to capture both local and global dependencies in the feature representations while maintaining gradient flow through residual connections.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "727fb05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One Transformer encoder block\n",
    "@tf.keras.utils.register_keras_serializable(package=\"Custom\")\n",
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads=8, mlp_dim=2048, dropout=0.1, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.mlp_dim   = mlp_dim\n",
    "        self.dropout   = dropout\n",
    "        self.mha  = layers.MultiHeadAttention(num_heads, key_dim=embed_dim)\n",
    "        self.norm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.mlp = tf.keras.Sequential([\n",
    "            layers.Dense(mlp_dim, activation=\"gelu\"),\n",
    "            layers.Dropout(dropout),\n",
    "            layers.Dense(embed_dim),\n",
    "            layers.Dropout(dropout)\n",
    "        ])\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.norm1(x + self.mha(x, x))\n",
    "        return self.norm2(x + self.mlp(x))\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"embed_dim\":  self.embed_dim,\n",
    "            \"num_heads\":  self.num_heads,\n",
    "            \"mlp_dim\":    self.mlp_dim,\n",
    "            \"dropout\":    self.dropout,\n",
    "        })\n",
    "        return {**config}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ff6478",
   "metadata": {},
   "source": [
    "## Hybrid model builder function\n",
    "\n",
    "Now, you will define a function `build_cnn_vit_hybrid` that constructs the complete hybrid CNN-Vision Transformer model. This function represents the main architectural innovation of the notebook, combining the strengths of convolutional neural networks for local feature extraction with transformers for global context modeling.\n",
    "\n",
    "**Function architecture:**\n",
    "1. **CNN feature extraction:** Extracts intermediate feature maps from a pre-trained CNN at a specified layer\n",
    "2. **Tokenization:** Reshapes spatial feature maps into a sequence of tokens suitable for transformer processing\n",
    "3. **Positional encoding:** Adds learnable positional embeddings to maintain spatial relationships\n",
    "4. **Transformer stack:** Applies multiple transformer encoder blocks for global context modeling\n",
    "5. **Classification head:** Pools tokens and applies the final classification layer\n",
    "\n",
    "**Parameters:**\n",
    "- `cnn_model`: Pre-trained CNN model for feature extraction\n",
    "- `feature_layer_name`: Name of the CNN layer to extract features from (e.g., `batch_normalization_5` in the original model architecture)\n",
    "- `num_transformer_layers`: Number of transformer blocks to stack (default 4)\n",
    "- `num_heads`: Number of attention heads per transformer block (default 8)\n",
    "- `mlp_dim`: MLP hidden dimension in transformer blocks (default 2048)\n",
    "- `num_classes`: Number of output classes for classification\n",
    "\n",
    "The function first freezes the CNN backbone (`cnn_model.trainable = False`) to use it as a fixed feature extractor. It then extracts feature maps with shape (B, H, W, C) and reshapes them to (B, H*W, C), where each spatial location becomes a token. The `AddPositionEmbedding` layer adds spatial awareness, and multiple TransformerBlock layers process the tokens. Finally, `GlobalAveragePooling1D` aggregates all tokens, and a dense layer with softmax activation produces class predictions.\n",
    "\n",
    "This hybrid approach leverages CNN's local feature detection capabilities while adding the transformer's global attention mechanism. The result is a model that can capture both fine-grained local patterns and long-range spatial dependencies, potentially **outperforming pure CNN** or pure transformer approaches on vision tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4c2598f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn_vit_hybrid(\n",
    "        cnn_model,\n",
    "        feature_layer_name,\n",
    "        num_transformer_layers=4,\n",
    "        num_heads=8,\n",
    "        mlp_dim=2048,\n",
    "        num_classes=2):\n",
    "    # 1. Freeze or fine-tune the CNN as you prefer\n",
    "    cnn_model.trainable = False      # set True to fine-tune\n",
    "    \n",
    "    # 2. Feature extractor up to the chosen layer\n",
    "    features = cnn_model.get_layer(feature_layer_name).output\n",
    "    H, W, C = features.shape[1], features.shape[2], features.shape[3]\n",
    "    \n",
    "    # 3. Flatten spatial grid → tokens  &  add positional encoding\n",
    "    x = layers.Reshape((H * W, C))(features) \n",
    "    x = AddPositionEmbedding(H * W, C)(x)\n",
    "\n",
    "    # 4. Stack ViT encoder blocks\n",
    "    for _ in range(num_transformer_layers):\n",
    "        x = TransformerBlock(C, num_heads, mlp_dim)(x)\n",
    "\n",
    "    # 5. Token pooling & classification head\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
    "\n",
    "    return Model(cnn_model.layers[0].input, outputs, name=\"CNN_ViT_hybrid\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46c3ff0",
   "metadata": {},
   "source": [
    "## Data generator configuration\n",
    "\n",
    "Now you will set up data preprocessing and augmentation pipeline using Keras' `ImageDataGenerator`.\n",
    "\n",
    "**Data configuration parameters:**\n",
    "- `img_w, img_h = 64, 64`: Input image dimensions (64x64 pixels)\n",
    "- `n_channels = 3`: RGB color channels\n",
    "- `batch_size = 128`: Number of samples per training batch\n",
    "- `num_classes = 2`: Binary classification setup\n",
    "\n",
    "**Generators:**\n",
    "Two separate generators are created:\n",
    "1. `train_gen`: Training data with augmentation and shuffling\n",
    "2. `val_gen`: Validation data with the same preprocessing but a different subset\n",
    "\n",
    "Both generators use `class_mode=\"categorical\"` for one-hot encoded labels, `target_size=(64,64)` for consistent input dimensions, and `shuffle=True` for randomized batch sampling.\n",
    "\n",
    "This augmentation strategy significantly increases the effective dataset size and helps prevent overfitting by exposing the model to varied versions of the same images. The validation split ensures proper model evaluation on unseen data, while the categorical class mode prepares labels for softmax classification in the hybrid model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5cc8c443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./images_dataSAT\n",
      "Found 4800 images belonging to 2 classes.\n",
      "Found 1200 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dataset_path = os.path.join(data_dir, \"images_dataSAT\")\n",
    "print(dataset_path)\n",
    "\n",
    "img_w, img_h = 64, 64\n",
    "n_channels = 3\n",
    "batch_size = 4\n",
    "num_classes = 2\n",
    "\n",
    "\n",
    "datagen = ImageDataGenerator(rescale=1./255,\n",
    "                             rotation_range=40, \n",
    "                             width_shift_range=0.2,\n",
    "                             height_shift_range=0.2,\n",
    "                             shear_range=0.2,\n",
    "                             zoom_range=0.2,\n",
    "                             horizontal_flip=True,\n",
    "                             fill_mode=\"nearest\",\n",
    "                             validation_split=0.2\n",
    "                            )\n",
    "\n",
    "train_gen = datagen.flow_from_directory(dataset_path,\n",
    "                                        target_size = (img_w, img_h),\n",
    "                                        batch_size= batch_size,\n",
    "                                        class_mode=\"categorical\",\n",
    "                                        subset=\"training\",\n",
    "                                        shuffle=True\n",
    "                                       )\n",
    "\n",
    "val_gen = datagen.flow_from_directory(dataset_path,\n",
    "                                      target_size =(img_w, img_h),\n",
    "                                      batch_size = batch_size, \n",
    "                                      class_mode=\"categorical\",\n",
    "                                      subset=\"validation\",\n",
    "                                      shuffle=True\n",
    "                                     )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b20ce4",
   "metadata": {},
   "source": [
    "## Model checkpoint setup\n",
    "\n",
    "This code cell configures a `ModelCheckpoint` callback for saving the best model weights during training. Model checkpointing is a crucial practice in deep learning that prevents loss of training progress and enables recovery of the best-performing model state.\n",
    "\n",
    "**Checkpoint configuration:**\n",
    "- `filepath`: Specifies the file path and name for saving weights\n",
    "- `save_weights_only=True`: Saves only model weights, not the full model architecture (more efficient and avoids serialization issues)\n",
    "- `monitor='val_accuracy'`: Tracks validation accuracy as the metric for determining the \"best\" model\n",
    "- `mode='max'`: Indicates that higher validation accuracy values are better (use 'min' for loss metrics)\n",
    "- `save_best_only=True`: Only saves the model when validation accuracy improves, preventing storage of worse-performing checkpoints\n",
    "- `verbose=1`: Provides console output when a checkpoint is saved\n",
    "\n",
    "The checkpoint callback addresses several important training considerations:\n",
    "1. **Overfitting prevention:** Captures the model state at peak validation performance before overfitting occurs\n",
    "2. **Storage efficiency:** Saving weights only reduces file size compared to full model serialization\n",
    "3. **Automatic model saving:** Eliminates manual monitoring by automatically saving the best-performing epoch\n",
    "\n",
    "**Integration with training:**\n",
    "This callback will be passed to the `model.fit()` method, where it will monitor validation accuracy after each epoch. When validation accuracy improves, the callback saves the current model weights to the specified file. This ensures that even if training continues past the optimal point, the best-performing weights are preserved.\n",
    "\n",
    "**File naming convention:**\n",
    "The filename uses the `.model.keras` extension to indicate it contains the full model architecture.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e30bfce4-40cf-4cbc-87ca-a3ffe0c176e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomPrintCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        # Print epoch number and starting time\n",
    "        print(f\"Epoch {(epoch + 1):02d} completed on {present_time()}\")\n",
    "time_print_callback = CustomPrintCallback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5a6d3120",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"keras_cnn_vit.model.keras\"\n",
    "# Save only weights to overcome the serialization issues with the hybrid model. The full model can be saved using the model architecture and weights.\n",
    "checkpoint_cb = ModelCheckpoint(filepath=model_name,\n",
    "                                save_weights_only=False,  # Set to True to save only weights\n",
    "                                monitor='val_loss',      # or 'val_accuracy', 'val_loss'\n",
    "                                mode='min',              # 'min' for loss, 'max' for accuracy\n",
    "                                save_best_only=True,\n",
    "                                verbose=1\n",
    "                                \n",
    "                               )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d310c72",
   "metadata": {},
   "source": [
    "## Model training and compilation\n",
    "\n",
    "Now, you will set up the core training pipeline, where the hybrid CNN-ViT model is built, compiled, and trained. This is the complete workflow from model instantiation to training execution with proper configuration for multi-class classification.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2596da50-d549-4f2a-a4b1-152291141de4",
   "metadata": {},
   "source": [
    "## Task 3: Define the model architecture in a variable named `hybrid_model` using the `build_cnn_vit_hybrid` function\n",
    "You may use the following parameters:\n",
    "\n",
    "- feature_layer_name: feature_layer_name\n",
    "- num_transformer_layers: 4\n",
    "- attention heads: 8\n",
    "- mlp dimension: 2048\n",
    "- num_classes: extract from training data generator (train_gen.num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f5abd941-a279-43bf-b4e7-0a89ffc922a0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cnn_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m num_classes \u001b[38;5;241m=\u001b[39m train_gen\u001b[38;5;241m.\u001b[39mnum_classes\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Criar o modelo híbrido CNN + ViT\u001b[39;00m\n\u001b[1;32m      5\u001b[0m hybrid_model \u001b[38;5;241m=\u001b[39m build_cnn_vit_hybrid(\n\u001b[0;32m----> 6\u001b[0m     \u001b[43mcnn_model\u001b[49m,\n\u001b[1;32m      7\u001b[0m     feature_layer_name\u001b[38;5;241m=\u001b[39mfeature_layer_name,\n\u001b[1;32m      8\u001b[0m     num_transformer_layers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m,\n\u001b[1;32m      9\u001b[0m     num_heads\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m,\n\u001b[1;32m     10\u001b[0m     mlp_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2048\u001b[39m,\n\u001b[1;32m     11\u001b[0m     num_classes\u001b[38;5;241m=\u001b[39mnum_classes\n\u001b[1;32m     12\u001b[0m )\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Visualizar o resumo do modelo\u001b[39;00m\n\u001b[1;32m     15\u001b[0m hybrid_model\u001b[38;5;241m.\u001b[39msummary()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cnn_model' is not defined"
     ]
    }
   ],
   "source": [
    "# Número de classes do generator de treino\n",
    "num_classes = train_gen.num_classes\n",
    "\n",
    "# Criar o modelo híbrido CNN + ViT\n",
    "hybrid_model = build_cnn_vit_hybrid(\n",
    "    cnn_model,\n",
    "    feature_layer_name=feature_layer_name,\n",
    "    num_transformer_layers=4,\n",
    "    num_heads=8,\n",
    "    mlp_dim=2048,\n",
    "    num_classes=num_classes\n",
    ")\n",
    "\n",
    "# Visualizar o resumo do modelo\n",
    "hybrid_model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe987db-3731-4866-87d4-69c440feffca",
   "metadata": {},
   "source": [
    "Double-click **here** for the solution.\n",
    "<!--\n",
    "num_classes = train_gen.num_classes\n",
    "hybrid_model = build_cnn_vit_hybrid(\n",
    "        cnn_model,\n",
    "        feature_layer_name=feature_layer_name,\n",
    "        num_transformer_layers=4,\n",
    "        num_heads=8,\n",
    "        mlp_dim=2048,\n",
    "        num_classes=train_gen.num_classes)\n",
    "\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47f907b-ec3b-49b8-ae94-c4944bad334d",
   "metadata": {},
   "source": [
    "## Task 4: Compile the model `hybrid_model` \n",
    "\n",
    "You may use the following parameters:\n",
    "- `optimizer=tf.keras.optimizers.Adam`\n",
    "- `learning rate: 0.0001`\n",
    "- `loss: categorical_crossentropy`\n",
    "- `metrics: accuracy`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "43445bc3-e606-4efd-9875-59d8578d69f6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'hybrid_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Compilar o modelo híbrido\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[43mhybrid_model\u001b[49m\u001b[38;5;241m.\u001b[39mcompile(\n\u001b[1;32m      5\u001b[0m     optimizer\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdam(\u001b[38;5;241m1e-4\u001b[39m),\n\u001b[1;32m      6\u001b[0m     loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcategorical_crossentropy\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      7\u001b[0m     metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      8\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'hybrid_model' is not defined"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Compilar o modelo híbrido\n",
    "hybrid_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191748d2-897b-44c8-993e-e5f1697d98f3",
   "metadata": {},
   "source": [
    "Double-click **here** for the solution.\n",
    "<!--\n",
    "hybrid_model.compile(optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "                     loss=\"categorical_crossentropy\",\n",
    "                     metrics=[\"accuracy\"],\n",
    "                    )\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4c5d3f",
   "metadata": {},
   "source": [
    "## Model shape validation\n",
    "\n",
    "This code cell performs the validation step to ensure the hybrid model produces outputs with the correct shape and dimensions. Shape validation is essential in deep learning to catch architectural errors early and verify that the model will work correctly with the expected input and output formats.\n",
    "\n",
    "**Validation process:**\n",
    "The cell creates a dummy input tensor using `tf.random.normal([1, img_w, img_h, n_channels])`, which generates random values with the same shape as actual input images:\n",
    "- Batch size: 1 (single sample for testing)\n",
    "- Width: `img_w` (64 pixels)\n",
    "- Height: `img_h` (64 pixels)\n",
    "- Channels: `n_channels` (3 for RGB)\n",
    "\n",
    "**Output verification:**\n",
    "The dummy input is passed through the hybrid model (`hybrid_model(dummy)`) to generate predictions. The expected output shape should be `(1, num_classes)` where:\n",
    "- First dimension (1): Batch size\n",
    "- Second dimension (`num_classes`): Number of classification classes\n",
    "\n",
    "**Technical benefits:**\n",
    "This validation step serves multiple purposes:\n",
    "1. **Architecture verification:** Confirms that all layers are properly connected and compatible\n",
    "2. **Dimension checking:** Ensures the model produces the expected output shape for classification\n",
    "3. **Early error detection:** Catches shape mismatches before actual training or inference\n",
    "4. **Model readiness:** Verifies the model is ready for production use\n",
    "\n",
    "**Importance:**\n",
    "If the output shape doesn't match expectations, it indicates potential issues in the hybrid architecture, such as incorrect reshaping operations, wrong number of classes configuration, or problems in the CNN-to-transformer transition. This simple test can save significant debugging time by catching architectural issues immediately after model construction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "22cbcd1d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'hybrid_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Ensure end-to-end shapes line up\u001b[39;00m\n\u001b[1;32m      2\u001b[0m dummy \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mnormal([\u001b[38;5;241m1\u001b[39m, img_w, img_h, n_channels])\n\u001b[0;32m----> 3\u001b[0m pred  \u001b[38;5;241m=\u001b[39m \u001b[43mhybrid_model\u001b[49m(dummy)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLogits shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, pred\u001b[38;5;241m.\u001b[39mshape)   \u001b[38;5;66;03m# should be (1, num_classes)\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'hybrid_model' is not defined"
     ]
    }
   ],
   "source": [
    "# Ensure end-to-end shapes line up\n",
    "dummy = tf.random.normal([1, img_w, img_h, n_channels])\n",
    "pred  = hybrid_model(dummy)\n",
    "print(\"Logits shape:\", pred.shape)   # should be (1, num_classes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1187a326-5472-4ade-b14d-3baf9992ae8a",
   "metadata": {},
   "source": [
    "## Task 5: Define the training configuration of the `hybrid_model`.\n",
    "In the interest of time, you can train for 3 epochs.\n",
    "Use the `checkpoint_cb` callback keyword for automatic saving of the best model state. \n",
    "\n",
    "To make sure that the computational resources are not overloaded, we will limit the number of batches used for training in each epoch. This can be done by **`steps_per_epoch`**. \n",
    "\n",
    "For this task use  **`steps_per_epoch = 128`**\n",
    "\n",
    "Feel free to play with these parameters if you are executing this on your local machine or any other platform.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ccf54504-7295-4db0-bd24-654b05a931c4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'hybrid_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m steps_per_epoch \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m128\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Treinar o modelo híbrido\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mhybrid_model\u001b[49m\u001b[38;5;241m.\u001b[39mfit(\n\u001b[1;32m      7\u001b[0m     train_gen,                 \u001b[38;5;66;03m# seu generator de treino\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     steps_per_epoch\u001b[38;5;241m=\u001b[39msteps_per_epoch,\n\u001b[1;32m      9\u001b[0m     epochs\u001b[38;5;241m=\u001b[39mepochs,\n\u001b[1;32m     10\u001b[0m     validation_data\u001b[38;5;241m=\u001b[39mval_gen,   \u001b[38;5;66;03m# seu generator de validação\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39m[checkpoint_cb]  \u001b[38;5;66;03m# salva o melhor modelo automaticamente\u001b[39;00m\n\u001b[1;32m     12\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'hybrid_model' is not defined"
     ]
    }
   ],
   "source": [
    "# Definir parâmetros de treinamento\n",
    "epochs = 3\n",
    "steps_per_epoch = 128\n",
    "\n",
    "# Treinar o modelo híbrido\n",
    "history = hybrid_model.fit(\n",
    "    train_gen,                 # seu generator de treino\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    epochs=epochs,\n",
    "    validation_data=val_gen,   # seu generator de validação\n",
    "    callbacks=[checkpoint_cb]  # salva o melhor modelo automaticamente\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81bcb41b-5739-47e9-817d-b8ea7a32b7ff",
   "metadata": {},
   "source": [
    "Double-click **here** for the solution.\n",
    "<!--\n",
    "\n",
    "fit = hybrid_model.fit(train_gen,\n",
    "                       epochs=3,\n",
    "                       validation_data=val_gen,\n",
    "                       callbacks=[checkpoint_cb],\n",
    "                       steps_per_epoch = 128\n",
    "                        )\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e06e06",
   "metadata": {},
   "source": [
    "## Training results visualization\n",
    "\n",
    "This code cell creates comprehensive visualizations of the training process by plotting both accuracy and loss curves. Visualization of training metrics is essential for understanding model performance, diagnosing training issues, and making informed decisions about model optimization.\n",
    "\n",
    "**Visualization setup:**\n",
    "The cell uses matplotlib to create two separate plots with consistent styling:\n",
    "- `fig_w, fig_h`: Sets figure dimensions for compact, readable plots\n",
    "- `plt.subplots(figsize=(fig_w, fig_h))`: Creates a subplot with specified dimensions\n",
    "\n",
    "**Accuracy plot analysis:**\n",
    "The first plot displays training and validation accuracy over epochs:\n",
    "- `fit.history['accuracy']`: Training accuracy progression\n",
    "- `fit.history['val_accuracy']`: Validation accuracy progression\n",
    "\n",
    "**Loss plot analysis:**\n",
    "The second plot shows training and validation loss curves:\n",
    "- `fit.history['loss']`: Training loss progression\n",
    "- `fit.history['val_loss']`: Validation loss progression\n",
    "\n",
    "***Importance:***\n",
    "These plots enable several important analyses:\n",
    "1. **Overfitting detection:** Diverging training and validation curves indicate overfitting\n",
    "2. **Convergence assessment:** Plateauing curves suggest training completion\n",
    "3. **Learning rate evaluation:** Oscillating curves may indicate learning rate issues\n",
    "4. **Model performance:** Final accuracy and loss values indicate overall model quality\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "25c26fab",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m fig, axs \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(figsize\u001b[38;5;241m=\u001b[39m(fig_w, fig_h ))\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Plot Accuracy on the first subplot\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m axs\u001b[38;5;241m.\u001b[39mplot(\u001b[43mfit\u001b[49m\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m], label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining Accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      7\u001b[0m axs\u001b[38;5;241m.\u001b[39mplot(fit\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m], label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mValidation Accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      8\u001b[0m axs\u001b[38;5;241m.\u001b[39mset_title(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mModel Accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'fit' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a figure with a subplot\n",
    "fig_w, fig_h = 4,3\n",
    "fig, axs = plt.subplots(figsize=(fig_w, fig_h ))\n",
    "\n",
    "# Plot Accuracy on the first subplot\n",
    "axs.plot(fit.history['accuracy'], label='Training Accuracy')\n",
    "axs.plot(fit.history['val_accuracy'], label='Validation Accuracy')\n",
    "axs.set_title('Model Accuracy')\n",
    "axs.set_xlabel('Epochs')\n",
    "axs.set_ylabel('Accuracy')\n",
    "axs.legend()\n",
    "axs.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "## You can use this cell to type the code to complete the task.\n",
    "\n",
    "fig, axs = plt.subplots(figsize=(fig_w, fig_h ))\n",
    "\n",
    "# Plot Loss on the second subplot\n",
    "axs.plot(fit.history['loss'], label='Training Loss')\n",
    "axs.plot(fit.history['val_loss'], label='Validation Loss')\n",
    "axs.set_title('Model Loss')\n",
    "axs.set_xlabel('Epochs')\n",
    "axs.set_ylabel('Loss')\n",
    "axs.legend()\n",
    "axs.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a580f2-7f09-4392-a807-b53419d4c8b1",
   "metadata": {},
   "source": [
    "## Save and download the trained model weights\n",
    "\n",
    "You have successfully trained the ViT model for classification of agricultural land from satellite imagery using **Keras**\n",
    "In this lab, in the interest of time, you have trained the model for 3-5 epochs. However, usually you need to train the model for around 15-20 epochs, depending on the quality of training data and model metrics based on validation. \n",
    "\n",
    "For your convenience, I have saved a model state dict for the model trained over 20 epochs **[here](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/7uNMQhNyTA8qSSDGn5Cc7A/keras-cnn-vit-ai-capstone.keras)**. You can download that for evaluation and further labs on your local machine from **[this link](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/7uNMQhNyTA8qSSDGn5Cc7A/keras-cnn-vit-ai-capstone.keras)**.\n",
    "\n",
    "\n",
    "Otherwise, you have also saved the model state dictionary for the best model using the `checkpoint_cb` callback function during training in this lab.\n",
    "\n",
    "You can also download the model state dict for the model that you have just trained for use in the subsequent labs.\n",
    "\n",
    "This is the PyTorch AI model state that can now be used for infering un-classified images. \n",
    "\n",
    "- You can download the trained model weights: `keras_cnn_vit.model.keras` from the left pane and save it on your local computer. \n",
    "- You can download this model by \"right-click\" on the file and then Clicking \"Download\".\n",
    "- In conjunction with the model architecture, these model weights can be used in other labs of this AI capstone course, instead of the weights provided at the above link\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02fe9d4e-2621-49cd-9118-df88597f36ce",
   "metadata": {},
   "source": [
    "## Save and download the notebook for **final project** submission and evaluation\n",
    "\n",
    "You will need to save and download the completed notebook for final project submission and evaluation. \n",
    "<br>For saving and downloading the completed notebook, please follow the steps given below:</br>\n",
    "\n",
    "<font size = 4>  \n",
    "\n",
    "1) **Complete** all the tasks and questions given in the notebook.\n",
    "\n",
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/nv4jHlPU5_R1q7ZJrZ69eg/DL0321EN-M1L1-Save-IPYNB-Screenshot-1.png\" style=\"width:600px; border:0px solid black;\">\n",
    "\n",
    "2) **Save** the notebook.</style>\n",
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/9-WPWD4mW1d-RV5Il5otTg/DL0321EN-M1L1-Save-IPYNB-Screenshot-2.png\" style=\"width:600px; border:0px solid black;\">\n",
    "\n",
    "3) Identify and right click on the **correct notebook file** in the left pane.</style>\n",
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/RUSRPw7NT6Sof94B7-9naQ/DL0321EN-M1L1-Save-IPYNB-Screenshot-3.png\" style=\"width:600px; border:0px solid black;\">\n",
    "\n",
    "4) Click on **Download**.</style>\n",
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/HHry4GT-vhLEcRi1T_LHGg/DL0321EN-M1L1-Save-IPYNB-Screenshot-4.png\" style=\"width:600px; border:0px solid black;\">\n",
    "\n",
    "5) Download and **Save** the Jupyter notebook file on your computer **for final submission**.</style>\n",
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/hhsJbxc6R-T8_pXQGjMjvg/DL0321EN-M1L1-Save-IPYNB-Screenshot-5.png\" style=\"width:600px; border:0px solid black;\">\n",
    "  </font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c4ff47-d95c-400f-ae87-56ca79b2ba98",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Congratulations! You've successfully built a CNN-ViT hybrid image classification model.\n",
    "In this lab, you learnt how to combine a convolutional neural network (CNN) with a Vision Transformer (ViT) for advanced image classification tasks. Starting from a pre-trained CNN, you learnt how to extract intermediate features, reshape them as tokens, and provide them with positional embeddings. By stacking transformer encoder blocks on top, the model benefits from both local detail extraction and global context awareness. Throughout the lab, techniques for robust data preparation, efficient training with model checkpoints, and effective visualization of performance were covered. By completing the steps in this notebook, you now have hands-on experience implementing and evaluating a contemporary hybrid vision model using Keras.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca5057e-a8f6-478d-8639-fd70fee4f8eb",
   "metadata": {},
   "source": [
    "<h2>Author</h2>\n",
    "\n",
    "[Aman Aggarwal](https://www.linkedin.com/in/aggarwal-aman)\n",
    "\n",
    "Aman Aggarwal is a PhD working at the intersection of neuroscience, AI, and drug discovery. He specializes in quantitative microscopy and image processing.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e075dc2f-6ffa-45a6-b2d8-860217305244",
   "metadata": {},
   "source": [
    "<!--\n",
    "## Change Log\n",
    "\n",
    "|  Date (YYYY-MM-DD) |  Version | Changed By  |  Change Description |\n",
    "|---|---|---|---|\n",
    "| 2025-07-14  | 1.0  | Aman  |  Created the lab |\n",
    "\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917371aa-f1b6-469e-b57f-cbb963d3eef7",
   "metadata": {},
   "source": [
    "© Copyright IBM Corporation. All rights reserved.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "prev_pub_hash": "61f2830ce2097e8820ce6407e6567640ed4feb54a55a17665fb7f3f613234ff1"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
