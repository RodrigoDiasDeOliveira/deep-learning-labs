{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6498861f-52ef-4ba3-bbbe-9259792a610f",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "  <a href=\"https://cognitiveclass.ai/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0321ENSkillsNetwork951-2022-01-01\">\n",
    "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DL0321EN-SkillsNetwork/image/IDSN-logo.png\" width=\"400\">\n",
    "  </a>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cdf6b95-f975-42df-b63b-f59078b5cd51",
   "metadata": {},
   "source": [
    "<h1 align=left><font size = 6>Lab: Comparative Analysis of Keras and PyTorch Models </font></h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f13f6f80-948d-4eb5-bd73-e328fc644dda",
   "metadata": {},
   "source": [
    "<h5>Estimated time: 90 minutes</h5>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2b5aad",
   "metadata": {},
   "source": [
    "<h2>Objective</h2>\n",
    "\n",
    "After completing this lab, you will be able to:\n",
    "<ul> \n",
    "\n",
    "1. Prepare data, load and evaluate Keras model.\n",
    "2. Prepare data, load and evaluate PyTorch model.\n",
    "3. Compute multiple performance metrics including accuracy, precision, recall, and f1-score.\n",
    "4. Visualize receiver operating characteristic (ROC) curves.\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4639ce32-38b5-410f-9c8f-7186738b8a38",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this lab, you will compare the performance of the Keras-based and the PyTorch based convolutional neural network (CNN) models using various evaluation metrics.  Common metrics include:\n",
    "\n",
    "- **Accuracy**: Measures how often the model is correct overall. A higher value means more total predictions are correct.\n",
    "\n",
    "- **Precision**: Measures how many predicted positives are actually correct. A higher value means fewer false positives (incorrectly predicted positives).\n",
    "\n",
    "- **Recall**: Measures how many real positives the model finds. A higher value means fewer false negatives (missed positive cases).\n",
    "\n",
    "- **F1 Score**: Tells us about the balance between precision and recall. A higher value means a better trade-off between precision and recall.\n",
    "\n",
    "- **ROC-AUC**: Measures the model’s ability to distinguish classes. A higher value reflects a model that can better distinguish between classes at all probability thresholds.\n",
    "\n",
    "\n",
    "For all these metrics, the model should aim for values as close to 1.0 (or 100%) as possible. Lower values indicate poorer model performance. There are exceptions for some metrics in other settings (like various loss functions, where lower is better), but for these standard classification metrics, higher is always better.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6219bc4-772b-4a74-88b1-4efe292b53b7",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "<font size = 3> \n",
    "    \n",
    "1. [Data download and extraction](#Data-download-and-extraction)\n",
    "2. [Package installation](#Package-installation)\n",
    "3. [Library imports and setup](#Library-imports-and-setup)\n",
    "4. [Evaluation metrics](#Evaluation-metrics)\n",
    "    1. [Accuracy](#1.-Accuracy)\n",
    "    2. [Precision](#2.-Precision)\n",
    "    3. [Recall](#3.-Recall-(sensitivity-or-true-positive-rate))\n",
    "    4. [F1 score](#4.-F1-score)\n",
    "    5. [Confusion matrix](#5.-Confusion-matrix)\n",
    "    6. [ROC-AUC](#6.-ROC-AUC-(Receiver-operating-characteristic---Area-under-curve))\n",
    "6. [Import the evaluation metrics](#Import-the-evaluation-metrics)\n",
    "7. [Model paths and download](#Model-paths-and-download)\n",
    "8. [Dataset path and parameters](#Dataset-path-and-parameters)\n",
    "9. [PyTorch model evaluation and prediction](#PyTorch-model-evaluation-and-prediction)\n",
    "10. [PyTorch metrics reporting](#PyTorch-metrics-reporting)\n",
    "11. [Keras model evaluation and prediction](#Keras-model-evaluation-and-prediction)\n",
    "12. [Keras metrics reporting](#Keras-metrics-reporting)\n",
    "13. [ROC curve plotting](#ROC-curve-plotting)\n",
    "14. [Comparing model performance](#Comparing-model-performance)\n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f18f62d",
   "metadata": {},
   "source": [
    "## Data download and extraction\n",
    "We begin by downloading the dataset for evaluation of the models.\n",
    "Here, you declare:\n",
    "1. The dataset URL from where the dataset would be downloaded.\n",
    "2. The dataset downloading primary function, based on `skillsnetwork` library.\n",
    "3. The dataset fallback downloading function, based on regular `http` downloading functions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a9f0820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write permissions available for downloading and extracting the dataset tar file\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40f7648096ef4cef9a83e14e827fe815",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading images-dataSAT.tar:   0%|          | 0/20243456 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dca86e8b8a5482d9f587a2315ec6c2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6003 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to '.'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import skillsnetwork\n",
    "\n",
    "data_dir = \".\"\n",
    "dataset_url = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/4Z1fwRR295-1O3PMQBH6Dg/images-dataSAT.tar\"\n",
    "\n",
    "\n",
    "def check_skillnetwork_extraction(extract_dir):\n",
    "    \"\"\"Check if the environment allows symlink creation for download/extraction.\"\"\"\n",
    "    symlink_test = os.path.join(extract_dir, \"symlink_test\")\n",
    "    if not os.path.exists(symlink_test):\n",
    "        os.symlink(os.path.join(os.sep, \"tmp\"), symlink_test)\n",
    "        print(\"Write permissions available for downloading and extracting the dataset tar file\")\n",
    "        os.unlink(symlink_test)\n",
    "\n",
    "async def download_tar_dataset(url, tar_path, extract_dir):\n",
    "    \"\"\"Download and extract dataset tar file asynchronously.\"\"\"\n",
    "    if not os.path.exists(tar_path):\n",
    "        try:\n",
    "            print(f\"Downloading from {url}...\")\n",
    "            import httpx\n",
    "            async with httpx.AsyncClient() as client:\n",
    "                response = await client.get(url, follow_redirects=True)\n",
    "                response.raise_for_status()\n",
    "                with open(tar_path, \"wb\") as f:\n",
    "                    f.write(response.content)\n",
    "            print(f\"Successfully downloaded '{tar_path}'.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Download error: {e}\")\n",
    "    else:\n",
    "        print(f\"Dataset tar file already exists at: {tar_path}\")\n",
    "    import tarfile\n",
    "    with tarfile.open(tar_path, 'r:*') as tar_ref:\n",
    "        tar_ref.extractall(path=extract_dir)\n",
    "        print(f\"Successfully extracted to '{extract_dir}'.\")\n",
    "\n",
    "try:\n",
    "    check_skillnetwork_extraction(data_dir)\n",
    "    await skillsnetwork.prepare(url=dataset_url, path=data_dir, overwrite=True)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    print(\"Primary download/extraction method failed.\")\n",
    "    print(\"Falling back to manual download and extraction...\")\n",
    "    import tarfile\n",
    "    import httpx\n",
    "    from pathlib import Path\n",
    "    file_name = Path(dataset_url).name\n",
    "    tar_path = os.path.join(data_dir, file_name)\n",
    "    await download_tar_dataset(dataset_url, tar_path, data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c1f18c",
   "metadata": {},
   "source": [
    "## Package installation\n",
    "\n",
    "Install the required basic Python packages. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c7ca672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 161 ms, sys: 93.6 ms, total: 255 ms\n",
      "Wall time: 50.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%capture captured_output\n",
    "%pip install numpy==1.26\n",
    "%pip install matplotlib==3.9.2\n",
    "%pip install skillsnetwork"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a28739c-8b88-4fc1-901b-7a3c24f8dd4f",
   "metadata": {},
   "source": [
    "### Install PyTorch library\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8a97ed4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch==2.7.0 in /opt/conda/lib/python3.12/site-packages (2.7.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0) (4.12.2)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0) (75.8.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0) (1.14.0)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0) (3.5)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0) (3.1.5)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0) (2025.9.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0) (9.5.1.17)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0) (2.26.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.3.0 in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0) (3.3.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.12/site-packages (from sympy>=1.13.3->torch==2.7.0) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.12/site-packages (from jinja2->torch==2.7.0) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "CPU times: user 80.2 ms, sys: 26.7 ms, total: 107 ms\n",
      "Wall time: 6.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%pip install torch==2.7.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a348399a-ee14-4050-a752-96f367f21b12",
   "metadata": {},
   "source": [
    "### Install PyTorch helper libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72a20f84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchvision==0.22\n",
      "  Downloading torchvision-0.22.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.12/site-packages (from torchvision==0.22) (1.26.0)\n",
      "Requirement already satisfied: torch==2.7.0 in /opt/conda/lib/python3.12/site-packages (from torchvision==0.22) (2.7.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.12/site-packages (from torchvision==0.22) (12.0.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0->torchvision==0.22) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0->torchvision==0.22) (4.12.2)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0->torchvision==0.22) (75.8.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0->torchvision==0.22) (1.14.0)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0->torchvision==0.22) (3.5)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0->torchvision==0.22) (3.1.5)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0->torchvision==0.22) (2025.9.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0->torchvision==0.22) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0->torchvision==0.22) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0->torchvision==0.22) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0->torchvision==0.22) (9.5.1.17)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0->torchvision==0.22) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0->torchvision==0.22) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0->torchvision==0.22) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0->torchvision==0.22) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0->torchvision==0.22) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0->torchvision==0.22) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0->torchvision==0.22) (2.26.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0->torchvision==0.22) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0->torchvision==0.22) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0->torchvision==0.22) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.3.0 in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0->torchvision==0.22) (3.3.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.12/site-packages (from sympy>=1.13.3->torch==2.7.0->torchvision==0.22) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.12/site-packages (from jinja2->torch==2.7.0->torchvision==0.22) (3.0.2)\n",
      "Downloading torchvision-0.22.0-cp312-cp312-manylinux_2_28_x86_64.whl (7.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m64.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Installing collected packages: torchvision\n",
      "  Attempting uninstall: torchvision\n",
      "    Found existing installation: torchvision 0.23.0+cpu\n",
      "    Uninstalling torchvision-0.23.0+cpu:\n",
      "      Successfully uninstalled torchvision-0.23.0+cpu\n",
      "Successfully installed torchvision-0.22.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "CPU times: user 148 ms, sys: 46.2 ms, total: 194 ms\n",
      "Wall time: 9.35 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%pip install torchvision==0.22"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd033f37-165f-41b8-90e2-a2439f035575",
   "metadata": {},
   "source": [
    "### Install tensorflow library for Keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d66b191c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow==2.19 in /opt/conda/lib/python3.12/site-packages (2.19.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.19) (2.3.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.19) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.19) (25.9.23)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.19) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.19) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.19) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.19) (3.4.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.19) (24.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.19) (5.29.5)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.19) (2.32.3)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.19) (75.8.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.19) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.19) (3.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.19) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.19) (2.0.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.19) (1.76.0)\n",
      "Requirement already satisfied: tensorboard~=2.19.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.19) (2.19.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.19) (3.12.0)\n",
      "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.19) (1.26.0)\n",
      "Requirement already satisfied: h5py>=3.11.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.19) (3.15.1)\n",
      "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.19) (0.5.4)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.12/site-packages (from astunparse>=1.6.0->tensorflow==2.19) (0.45.1)\n",
      "Requirement already satisfied: rich in /opt/conda/lib/python3.12/site-packages (from keras>=3.5.0->tensorflow==2.19) (14.2.0)\n",
      "Requirement already satisfied: namex in /opt/conda/lib/python3.12/site-packages (from keras>=3.5.0->tensorflow==2.19) (0.1.0)\n",
      "Requirement already satisfied: optree in /opt/conda/lib/python3.12/site-packages (from keras>=3.5.0->tensorflow==2.19) (0.18.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow==2.19) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow==2.19) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow==2.19) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow==2.19) (2024.12.14)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.12/site-packages (from tensorboard~=2.19.0->tensorflow==2.19) (3.10)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.12/site-packages (from tensorboard~=2.19.0->tensorflow==2.19) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.12/site-packages (from tensorboard~=2.19.0->tensorflow==2.19) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow==2.19) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.12/site-packages (from rich->keras>=3.5.0->tensorflow==2.19) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.12/site-packages (from rich->keras>=3.5.0->tensorflow==2.19) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow==2.19) (0.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "CPU times: user 28.2 ms, sys: 19.7 ms, total: 48 ms\n",
      "Wall time: 2.44 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%pip install tensorflow==2.19"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55c8f13-46b3-4e8e-a988-3af92750dfee",
   "metadata": {},
   "source": [
    "### Install SkLearn library for evaluation metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9829348b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn==1.7.0 in /opt/conda/lib/python3.12/site-packages (1.7.0)\n",
      "Requirement already satisfied: numpy>=1.22.0 in /opt/conda/lib/python3.12/site-packages (from scikit-learn==1.7.0) (1.26.0)\n",
      "Requirement already satisfied: scipy>=1.8.0 in /opt/conda/lib/python3.12/site-packages (from scikit-learn==1.7.0) (1.16.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/conda/lib/python3.12/site-packages (from scikit-learn==1.7.0) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/conda/lib/python3.12/site-packages (from scikit-learn==1.7.0) (3.6.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "CPU times: user 23.3 ms, sys: 9.8 ms, total: 33.1 ms\n",
      "Wall time: 1.47 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%pip install scikit-learn==1.7.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134c4ade",
   "metadata": {},
   "source": [
    "## Library imports and setup\n",
    "\n",
    "Import essential libraries for data manipulation, visualization, and suppresses warnings for cleaner notebook output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cd0fcdf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 480 ms, sys: 192 ms, total: 671 ms\n",
      "Wall time: 718 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "import time\n",
    "import httpx\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46c7a98",
   "metadata": {},
   "source": [
    "### PyTorch library imports\n",
    "\n",
    "Import core PyTorch modules for model building, optimization, data loading, and functional utilities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "92b7cb66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported libraries\n",
      "CPU times: user 3 s, sys: 703 ms, total: 3.7 s\n",
      "Wall time: 5.29 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "print(\"Imported libraries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a0a554",
   "metadata": {},
   "source": [
    "### TensorFlow/Keras library imports\n",
    "\n",
    "These imports set the environment variables to reduce TensorFlow logging noise and imports Keras modules for model building and training. They detect GPU availability for device assignment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "99804321",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1764003259.815297     838 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1764003259.822292     838 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1764003259.842356     838 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1764003259.842395     838 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1764003259.842397     838 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1764003259.842399     838 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device available for training: cpu\n",
      "CPU times: user 3.11 s, sys: 631 ms, total: 3.74 s\n",
      "Wall time: 4.33 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout, BatchNormalization\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.initializers import HeUniform\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "gpu_list = tf.config.list_physical_devices('GPU')\n",
    "device = \"gpu\" if gpu_list != [] else \"cpu\"\n",
    "print(f\"Device available for training: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ae515f",
   "metadata": {},
   "source": [
    "## Evaluation metrics \n",
    "\n",
    "The following metrics are used for evaluation of various AI/ML models:\n",
    "    \n",
    "- Accuracy\n",
    "- Precision\n",
    "- Recall\n",
    "- F1 score\n",
    "- Confusion matrix\n",
    "- Receiver Operating Characteristic - Area Under Curve (ROC-AUC)\n",
    "\n",
    "You can read about their calculation methods and their significance for model performance below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371e4b91-1b94-4563-a099-6a811a328f70",
   "metadata": {},
   "source": [
    "### 1. Accuracy\n",
    "\n",
    "**Definition:**\n",
    "Accuracy is the proportion of correct predictions (both true positives and true negatives) among the total number of cases examined. In other words, it measures how often the classifier is correct overall.\n",
    "\n",
    "**Formula:**\n",
    "\\[\n",
    "Accuracy = $\\frac{TP + TN}{TP + TN + FP + FN}$\n",
    "\\]\n",
    "\n",
    "- TP: True positives (correctly predicted positive cases)\n",
    "- TN: True negatives (correctly predicted negative cases)\n",
    "- FP: False positives (incorrectly predicted positive cases)\n",
    "- FN: False negatives (incorrectly predicted negative cases)\n",
    "\n",
    "**Significance:**\n",
    "\n",
    "Accuracy is intuitive and easy to interpret, making it a common first metric for model evaluation. However, it can be misleading if the dataset is imbalanced (i.e., one class is much more frequent than the other). This is because a model can achieve high accuracy by simply predicting the majority class.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879be2c6-dd4a-4efb-be7a-32a21ec3ba9d",
   "metadata": {},
   "source": [
    "### 2. Precision\n",
    "\n",
    "**Definition:**\n",
    "Precision measures the proportion of positive predictions that are actually correct. It answers the question: \"Of all the samples that the model predicted as positive, how many were truly positive?\"\n",
    "\n",
    "**Formula:**\n",
    "\\[\n",
    "Precision = $\\frac{TP}{TP + FP}$\n",
    "\\]\n",
    "\n",
    "**Significance:**\n",
    "Precision is crucial when the cost of a false positive is high. For example, in medical diagnosis, predicting a disease when it's not present (false positive) can lead to unnecessary treatments. In land classification, high precision means that when the model predicts a tile as agricultural, it is likely correct.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be151b6-8f48-4ec2-9cc9-0e7cf24feac5",
   "metadata": {},
   "source": [
    "### 3. Recall (sensitivity or true positive rate)\n",
    "\n",
    "**Definition:**\n",
    "Recall measures the proportion of actual positive cases that were correctly identified by the model. It answers: \"Of all the true positive samples, how many did the model identify?\"\n",
    "\n",
    "**Formula:**\n",
    "\\[\n",
    "Recall = $\\frac{TP}{TP + FN}$\n",
    "\\]\n",
    "\n",
    "**Significance:**\n",
    "Recall is important when the cost of missing a positive case (false negative) is high. In land classification, high recall means the model is good at finding all the agricultural land, even if it sometimes mislabels non-agricultural land as agricultural.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8eba49-ce14-4cef-9fb0-45c8a2a7931c",
   "metadata": {},
   "source": [
    "### 4. F1 score\n",
    "\n",
    "**Definition:**\n",
    "The F1 score is the harmonic mean of precision and recall. It provides a single metric that balances both concerns. It is especially useful when you need to find an equilibrium between precision and recall.\n",
    "\n",
    "**Formula:**\n",
    "\\[\n",
    "F1 = $2 \\times \\frac{Precision \\times Recall}{Precision + Recall}$\n",
    "\\]\n",
    "\n",
    "**Significance:**\n",
    "The F1 score is especially valuable when the class distribution is uneven or when both false positives and false negatives are important. It penalizes extreme values, so a model with high precision but low recall (or vice versa) will have a lower F1 score.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab48236-1003-4888-b84c-bcd2b5b385e0",
   "metadata": {},
   "source": [
    "### 5. Confusion matrix\n",
    "\n",
    "**Definition:**\n",
    "A confusion matrix is a table that summarizes the performance of a classification algorithm. It displays the counts of true positives, false positives, true negatives, and false negatives.\n",
    "\n",
    "|               | Predicted positive | Predicted negative |\n",
    "|---------------|-------------------|-------------------|\n",
    "| Actual positive | True positive (TP) | False negative (FN) |\n",
    "| Actual negative | False positive (FP) | True negative (TN) |\n",
    "\n",
    "**Significance:**\n",
    "The confusion matrix provides a detailed breakdown of model errors and successes, helping you understand not just how often the model is right, but *how* it is wrong. This is crucial for diagnosing issues like class imbalance or systematic misclassification.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39b5a8b-6836-4fb7-86db-36c6b8efb675",
   "metadata": {},
   "source": [
    "### 6. ROC-AUC (Receiver operating characteristic - Area under curve)\n",
    "\n",
    "**Definition:**\n",
    "ROC-AUC measures the model's ability to distinguish between classes across all possible classification thresholds. The ROC curve plots the true positive rate (recall) against the false positive rate at various thresholds. The AUC (area under the curve) summarizes this performance in a single value between 0 and 1.\n",
    "\n",
    "**Significance:**\n",
    "A model with an ROC-AUC of 1.0 perfectly distinguishes between classes, while a value of 0.5 suggests random guessing. ROC-AUC is especially useful for imbalanced datasets and when you care about the ranking of predictions rather than their absolute values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51947c1-a446-45f4-83d7-b9d8472847e4",
   "metadata": {},
   "source": [
    "## Import the evaluation metrics\n",
    "\n",
    "Here you define the functions to compute and print classification metrics including accuracy, precision, recall, F1 score, ROC-AUC, confusion matrix, and log loss. These functions support both Keras and PyTorch model outputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a2ab94b1-7072-4d1f-8838-cf14c99a17a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 29.7 ms, sys: 12.6 ms, total: 42.3 ms\n",
      "Wall time: 50.2 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.metrics import (accuracy_score,\n",
    "                             precision_score,\n",
    "                             recall_score,\n",
    "                             f1_score,\n",
    "                             roc_curve, \n",
    "                             roc_auc_score,\n",
    "                             log_loss,\n",
    "                             classification_report,\n",
    "                             confusion_matrix,\n",
    "                             ConfusionMatrixDisplay,\n",
    "                            )\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "# define a function to get the metrics comprehensively\n",
    "def model_metrics(y_true, y_pred, y_prob, class_labels):\n",
    "    metrics = {'Accuracy': accuracy_score(y_true, y_pred),\n",
    "               'Precision': precision_score(y_true, y_pred),\n",
    "               'Recall': recall_score(y_true, y_pred),\n",
    "               'Loss': log_loss(y_true, y_prob),\n",
    "               'F1 Score': f1_score(y_true, y_pred),\n",
    "               'ROC-AUC': roc_auc_score(y_true, y_prob),\n",
    "               'Confusion Matrix': confusion_matrix(y_true, y_pred),\n",
    "               'Classification Report': classification_report(y_true, y_pred, target_names=class_labels, digits=4),\n",
    "               \"Class labels\": class_labels\n",
    "              }\n",
    "    return metrics\n",
    "\n",
    "#function to print the metrics\n",
    "def print_metrics(y_true, y_pred, y_prob, class_labels, model_name):\n",
    "    metrics = model_metrics(y_true, y_pred, y_prob, class_labels)\n",
    "    print(f\"Evaluation metrics for the \\033[1m{model_name}\\033[0m\")\n",
    "    print(f\"Accuracy: {'':<1}{metrics[\"Accuracy\"]:.4f}\")\n",
    "    print(f\"ROC-AUC: {'':<2}{metrics[\"ROC-AUC\"]:.4f}\")\n",
    "    print(f\"Loss: {'':<5}{metrics[\"Loss\"]:.4f}\\n\")\n",
    "    print(f\"Classification report:\\n\\n  {metrics[\"Classification Report\"]}\")\n",
    "    print(\"========= Confusion Matrix =========\")\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=metrics[\"Confusion Matrix\"],\n",
    "                                  display_labels=metrics[\"Class labels\"])\n",
    "\n",
    "    disp.plot()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc53fe7",
   "metadata": {},
   "source": [
    "## Model download helper\n",
    "\n",
    "Now, define an asynchronous function to download model files from given URLs, if they are not already present locally. \n",
    "You use `httpx` for asynchronous HTTP requests with error handling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e211b54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def download_model(url, model_path):\n",
    "    if not os.path.exists(model_path):\n",
    "        try:\n",
    "            print(f\"Downloading from {url}...\")\n",
    "            import httpx\n",
    "            async with httpx.AsyncClient() as client:\n",
    "                response = await client.get(url, follow_redirects=True)\n",
    "                response.raise_for_status()\n",
    "                with open(model_path, \"wb\") as f:\n",
    "                    f.write(response.content)\n",
    "            print(f\"Successfully downloaded '{model_path}'.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Download error: {e}\")\n",
    "    else:\n",
    "        print(f\"Model file already downloaded at: {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e94bd9",
   "metadata": {},
   "source": [
    "## Model paths and download\n",
    "\n",
    "In the cell below, you define the file paths and URLs for the Keras and PyTorch models and download them using the `download_model` function defined above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "be04a5fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading from https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/U-uPeyCyOQYh0GrZPGsqoQ/ai-capstone-keras-best-model-model.keras...\n",
      "Successfully downloaded './ai-capstone-keras-best-model-model_downloaded.keras'.\n",
      "Downloading from https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/8J2QEyQqD8x9zjrlnv6N7g/ai-capstone-pytorch-best-model-20250713.pth...\n",
      "Successfully downloaded './ai_capstone_pytorch_best_model_state_dict_downloaded.pth'.\n"
     ]
    }
   ],
   "source": [
    "data_dir = \".\"\n",
    "\n",
    "keras_model_url = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/U-uPeyCyOQYh0GrZPGsqoQ/ai-capstone-keras-best-model-model.keras\"\n",
    "keras_model_name = \"ai-capstone-keras-best-model-model_downloaded.keras\"\n",
    "keras_model_path = os.path.join(data_dir, keras_model_name)\n",
    "\n",
    "pytorch_state_dict_url = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/8J2QEyQqD8x9zjrlnv6N7g/ai-capstone-pytorch-best-model-20250713.pth\"\n",
    "pytorch_state_dict_name = \"ai_capstone_pytorch_best_model_state_dict_downloaded.pth\"\n",
    "pytorch_state_dict_path = os.path.join(data_dir, pytorch_state_dict_name)\n",
    "\n",
    "await download_model(keras_model_url, keras_model_path)\n",
    "await download_model(pytorch_state_dict_url, pytorch_state_dict_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b6a821",
   "metadata": {},
   "source": [
    "## Dataset path and parameters\n",
    "\n",
    "Here, for downstream processing, you define \n",
    "1. the dataset directory path\n",
    "2. define image dimensions\n",
    "3. number of channels\n",
    "4. batch size\n",
    "5. number of classes\n",
    "6. class labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "87cfea7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./images_dataSAT\n"
     ]
    }
   ],
   "source": [
    "dataset_path = os.path.join(data_dir, \"images_dataSAT\")\n",
    "print(dataset_path)\n",
    "\n",
    "img_w, img_h = 64, 64\n",
    "n_channels = 3\n",
    "batch_size = 128\n",
    "num_classes = 2\n",
    "\n",
    "agri_class_labels = [\"non-agri\", \"agri\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c2bc67",
   "metadata": {},
   "source": [
    "## Keras model evaluation and prediction\n",
    "\n",
    "In this cell, you will:\n",
    "- Use `ImageDataGenerator` to rescale images.\n",
    "- Load test images from the dataset directory.\n",
    "- Load the saved Keras model using `tf.keras.models.load_model`.\n",
    "- Run predictions on the test set, collect predicted probabilities, predicted classes, and true labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4fd7f5a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6000 images belonging to 2 classes.\n",
      "Number of Steps: 47 with batch size: 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Steps: 100%|██████████| 47/47 [02:08<00:00,  2.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 49s, sys: 7.47 s, total: 1min 57s\n",
      "Wall time: 2min 11s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "datagen = ImageDataGenerator(rescale=1./255)\n",
    "prediction_generator = datagen.flow_from_directory(\n",
    "    dataset_path,\n",
    "    target_size=(img_w, img_h),\n",
    "    batch_size=batch_size,\n",
    "    class_mode=\"binary\",\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "keras_model = tf.keras.models.load_model(keras_model_path)\n",
    "\n",
    "steps = int(np.ceil(prediction_generator.samples / prediction_generator.batch_size))\n",
    "batch_size = int(prediction_generator.batch_size)\n",
    "print(f\"Number of Steps: {steps} with batch size: {batch_size}\")\n",
    "\n",
    "all_preds_keras = []\n",
    "all_probs_keras = []\n",
    "all_labels_keras = []\n",
    "\n",
    "for step_idx, step in enumerate(tqdm(range(steps), desc=\"Steps\")):\n",
    "    images, labels = next(prediction_generator)\n",
    "    preds = keras_model.predict(images, verbose='0')\n",
    "    all_probs_keras.extend(preds)\n",
    "    preds = (preds > 0.5).astype(int).flatten()\n",
    "    all_preds_keras.extend(preds)\n",
    "    all_labels_keras.extend(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973d28b7-43ac-49c0-ac58-4b13fcf76fff",
   "metadata": {},
   "source": [
    "#### Question: What does the code **`preds > 0.5`** in line `preds = (preds > 0.5).astype(int).flatten()` do?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05026e2c-8187-4651-9dab-cbfe19b644ce",
   "metadata": {},
   "source": [
    "Please use the space below to write your answer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c5ad27-832d-4f14-87d4-9ef5b856af62",
   "metadata": {},
   "source": [
    "# Explicando de forma simples o que faz esta linha:\n",
    "# preds = (preds > 0.5).astype(int).flatten()\n",
    "\n",
    "# 1. preds > 0.5\n",
    "#    - Verifica cada número na lista preds (probabilidades entre 0 e 1)\n",
    "#    - Se for maior que 0.5, vira True; se não, False\n",
    "\n",
    "# 2. .astype(int)\n",
    "#    - Converte True em 1 e False em 0\n",
    "\n",
    "# 3. .flatten()\n",
    "#    - Transforma tudo em uma lista simples de números, tipo [0, 1, 0, 1, ...]\n",
    "\n",
    "# Resumindo:\n",
    "# Pega as probabilidades da rede, decide se é 0 ou 1 usando 0.5 como limite,\n",
    "# e retorna uma lista plana de 0s e 1s.\n",
    "\n",
    "# Exemplo ilustrativo:\n",
    "import numpy as np\n",
    "\n",
    "preds = np.array([0.2, 0.7, 0.4, 0.9])  # probabilidades de saída do modelo\n",
    "preds_binary = (preds > 0.5).astype(int).flatten()\n",
    "print(\"Predições binárias:\", preds_binary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6008b4c1-2d7d-4a93-9714-a687b3e29a5e",
   "metadata": {},
   "source": [
    "Double-click **here** for the solution.\n",
    "<!--\n",
    "\"It converts all predictions greater than 0.5 to True or assign to class 1. Rest of the predictions are False, assigned to class 0\"\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971389d2-f67f-4c21-a79a-2db089082422",
   "metadata": {},
   "source": [
    "## Keras metrics reporting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aec013b-f37f-4231-a33f-952650170d71",
   "metadata": {},
   "source": [
    "### Task 1: Print the performance metrics for the Keras model using `print_metrics` function\n",
    "\n",
    "Print various performance metrics for the **Keras** model. You may use the previously defined metrics print function `print_metrics`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "54ab980a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Supondo que o modelo Keras já foi treinado e temos os dados de validação:\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# X_val, y_val (ou validation data) disponíveis\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# E que a função print_metrics já está definida\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Obter as predições do modelo\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m preds \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mpredict(X_val)  \u001b[38;5;66;03m# ou validation_generator se estiver usando generator\u001b[39;00m\n\u001b[1;32m      7\u001b[0m preds_binary \u001b[38;5;241m=\u001b[39m (preds \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.5\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\u001b[38;5;241m.\u001b[39mflatten()  \u001b[38;5;66;03m# binarizar usando threshold 0.5\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Chamar a função de métricas para exibir resultados\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# Supondo que o modelo Keras já foi treinado e temos os dados de validação:\n",
    "# X_val, y_val (ou validation data) disponíveis\n",
    "# E que a função print_metrics já está definida\n",
    "\n",
    "# Obter as predições do modelo\n",
    "preds = model.predict(X_val)  # ou validation_generator se estiver usando generator\n",
    "preds_binary = (preds > 0.5).astype(int).flatten()  # binarizar usando threshold 0.5\n",
    "\n",
    "# Chamar a função de métricas para exibir resultados\n",
    "print_metrics(y_true=y_val, y_pred=preds_binary)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c2e854-e1c3-4d08-a0a3-eaa09340a389",
   "metadata": {},
   "source": [
    "Double-click **here** for the solution.\n",
    "<!--\n",
    "print_metrics(y_true = all_labels_keras,\n",
    "              y_pred = all_preds_keras,\n",
    "              y_prob = all_probs_keras,\n",
    "              class_labels = agri_class_labels,\n",
    "              model_name = \"Keras Model\"\n",
    "             )\n",
    "\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b613b4-911e-485c-9de7-f98b3c02e068",
   "metadata": {},
   "source": [
    "#### Question: What is the significance of `f1 score`?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5bfe96-2d87-46f1-9374-3d2755aae0c4",
   "metadata": {},
   "source": [
    "Please use the space below to write your answer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8b7c86-c3a3-40ec-bc53-de6df17dea29",
   "metadata": {},
   "source": [
    "Significance:\n",
    "\n",
    "Gives a single measure of model performance that considers both false positives and false negatives.\n",
    "\n",
    "High F1 score means the model is doing well in correctly predicting positives while minimizing false alarms.\n",
    "\n",
    "Essential for cases like medical diagnosis, fraud detection, or any task with imbalanced classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e571bbc-2708-4db9-b9ca-6573c0bd095d",
   "metadata": {},
   "source": [
    "\n",
    "Double-click **here** for the solution.\n",
    "<!--\n",
    "\"It is useful when both false positives and false negatives are important\"\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6202c22e",
   "metadata": {},
   "source": [
    "## PyTorch model evaluation and prediction\n",
    "\n",
    "In this cell, you:\n",
    "- Set device for inference (GPU if available).\n",
    "- Define data transformations including resizing, normalization.\n",
    "- Load the dataset using `ImageFolder` and prepares a DataLoader.\n",
    "- Define the CNN architecture matching the saved state dict.\n",
    "- Load model weights.\n",
    "- Run inference on the test set, collecting predicted classes, probabilities, and true labels for metric calculation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d240b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Processing inference on {device}\")\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((img_w, img_h)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "full_dataset = datasets.ImageFolder(dataset_path, transform=train_transform)\n",
    "test_loader = DataLoader(full_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Conv2d(3, 32, 5, padding=2), nn.ReLU(),\n",
    "    nn.MaxPool2d(2), nn.BatchNorm2d(32),\n",
    "    nn.Conv2d(32, 64, 5, padding=2), nn.ReLU(), nn.MaxPool2d(2), nn.BatchNorm2d(64),\n",
    "    nn.Conv2d(64, 128, 5, padding=2), nn.ReLU(), nn.MaxPool2d(2), nn.BatchNorm2d(128),\n",
    "    nn.Conv2d(128, 256, 5, padding=2), nn.ReLU(), nn.MaxPool2d(2), nn.BatchNorm2d(256),\n",
    "    nn.Conv2d(256, 512, 5, padding=2), nn.ReLU(), nn.MaxPool2d(2), nn.BatchNorm2d(512),\n",
    "    nn.Conv2d(512, 1024, 5, padding=2), nn.ReLU(), nn.MaxPool2d(2), nn.BatchNorm2d(1024),\n",
    "    nn.AdaptiveAvgPool2d(1), nn.Flatten(),\n",
    "    nn.Linear(1024, 2048), nn.ReLU(), nn.BatchNorm1d(2048), nn.Dropout(0.4),\n",
    "    nn.Linear(2048, num_classes)\n",
    ").to(device)\n",
    "\n",
    "print(\"Created model, now loading the weights from saved model state dict\")\n",
    "model.load_state_dict(torch.load(pytorch_state_dict_path))\n",
    "print(\"Loaded model state dict, now getting predictions\")\n",
    "\n",
    "all_preds_pytorch = []\n",
    "all_labels_pytorch = []\n",
    "all_probs_pytorch = []\n",
    "\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (images, labels) in enumerate(tqdm(test_loader, desc=\"Step\")):\n",
    "#    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        outputs = model(images)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        probs = F.softmax(outputs, dim=1)[:, 1]  # probability for class 1\n",
    "        all_probs_pytorch.extend(probs.cpu())\n",
    "        all_preds_pytorch.extend(preds.cpu().numpy().flatten())\n",
    "        all_labels_pytorch.extend(labels.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc86e4a-c572-439b-b23a-5e272299caf9",
   "metadata": {},
   "source": [
    "## PyTorch metrics reporting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5119d485-8fbd-4c4e-9405-505369288e48",
   "metadata": {},
   "source": [
    "### Task 2: Print the performance metrics for the PyTorch model using `print_metrics`\n",
    "\n",
    "Print various performance metrics for the PyTorch model. You may use the previously defined metrics print function `print_metrics`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47c2d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# Evaluate the PyTorch model and print metrics\n",
    "# ---------------------------\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Create lists to store true labels and predictions\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in val_loader:  # Use your validation DataLoader\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)  # Get predicted class indices\n",
    "        y_true.extend(labels.numpy())\n",
    "        y_pred.extend(predicted.numpy())\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "y_true = np.array(y_true)\n",
    "y_pred = np.array(y_pred)\n",
    "\n",
    "# Print the performance metrics using the defined function\n",
    "print_metrics(y_true, y_pred)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93c22ac-e1fa-489e-b886-8de9a7ff3848",
   "metadata": {},
   "source": [
    "Double-click **here** for the solution.\n",
    "<!--\n",
    "print_metrics(y_true = all_labels_pytorch,\n",
    "              y_pred = all_preds_pytorch,\n",
    "              y_prob = all_probs_pytorch,\n",
    "              class_labels = agri_class_labels,\n",
    "              model_name = \"PyTorch Model\"\n",
    "             )\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb0a22b-3cf1-4543-b012-77c192042eff",
   "metadata": {},
   "source": [
    "#### Question: What are the total number of false negatives in the `confusion matrix` in the PyTorch model evaluated above? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afcf7d8f-a2c8-4e4f-9744-ccaf46354f70",
   "metadata": {},
   "source": [
    "Please use the space below to write your answer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8b7d4a-463d-4b46-96d7-1e333f542933",
   "metadata": {},
   "source": [
    "0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c14f876e-6165-4553-90f3-fcdd9bd1d056",
   "metadata": {},
   "source": [
    "Double-click **here** for the solution.\n",
    "<!--\n",
    "\"Total Flase negatives are 5\"\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ebfcf7",
   "metadata": {},
   "source": [
    "## ROC curve plotting\n",
    "\n",
    "First, define a function to plot ROC curves for binary or multi-class classification using scikit-learn's `roc_curve` and `roc_auc_score`. It handles both single-class and multi-class cases by binarizing labels if needed.\n",
    "\n",
    "Next, plot the ROC curves for both the models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4bdd1da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_roc(y_true, y_prob, model_name):\n",
    "    n_classes = y_prob.shape[1] if y_prob.ndim > 1 else 1\n",
    "    if n_classes == 1:\n",
    "        fpr, tpr, _ = roc_curve(y_true, y_prob)\n",
    "        auc = roc_auc_score(y_true, y_prob)\n",
    "        plt.plot(fpr, tpr, label=f'{model_name} (AUC = {auc:.2f})')\n",
    "    else:\n",
    "        y_true_bin = label_binarize(y_true, classes=np.arange(n_classes))\n",
    "        for i in range(n_classes):\n",
    "            fpr, tpr, _ = roc_curve(y_true_bin[:, i], y_prob[:, i])\n",
    "            auc = roc_auc_score(y_true_bin[:, i], y_prob[:, i])\n",
    "            plt.plot(fpr, tpr, label=f'{model_name} class {i} (AUC = {auc:.2f})')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curve')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45792448",
   "metadata": {},
   "source": [
    "### ROC curve plotting for both models\n",
    "\n",
    "Plot the ROC curves for both Keras and PyTorch models on the same figure for visual performance comparison.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "097b5831",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'all_labels_pytorch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m plot_roc(np\u001b[38;5;241m.\u001b[39marray(all_labels_keras), np\u001b[38;5;241m.\u001b[39marray(all_probs_keras), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKeras Model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[0;32m----> 3\u001b[0m plot_roc(np\u001b[38;5;241m.\u001b[39marray(\u001b[43mall_labels_pytorch\u001b[49m), np\u001b[38;5;241m.\u001b[39marray(all_probs_pytorch), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPyTorch Model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'all_labels_pytorch' is not defined"
     ]
    }
   ],
   "source": [
    "plot_roc(np.array(all_labels_keras), np.array(all_probs_keras), \"Keras Model\")\n",
    "plt.show()\n",
    "plot_roc(np.array(all_labels_pytorch), np.array(all_probs_pytorch), \"PyTorch Model\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983e6e57-9764-44a3-8566-cef177c1e684",
   "metadata": {},
   "source": [
    "## Comparing model performance\n",
    "\n",
    "Now compare the performance of different models to understand which model would be the best performer for your land classification task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f871167c-5e80-4f64-bc89-106ffc63935b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_labels_pytorch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m metrics_keras \u001b[38;5;241m=\u001b[39m model_metrics(all_labels_keras, all_preds_keras, all_probs_keras, agri_class_labels)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# get the PyTorch model performance metrics\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m metrics_pytorch \u001b[38;5;241m=\u001b[39m model_metrics(\u001b[43mall_labels_pytorch\u001b[49m, all_preds_pytorch, all_probs_pytorch, agri_class_labels)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Display the comparison of metrics\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{:<18}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{:<15}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{:<15}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\033\u001b[39;00m\u001b[38;5;124m[1m\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMetric\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\033\u001b[39;00m\u001b[38;5;124m[0m\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     10\u001b[0m                                     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mKeras Model\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m     11\u001b[0m                                     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPyTorch Model\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'all_labels_pytorch' is not defined"
     ]
    }
   ],
   "source": [
    "# get the Keras model performance metrics\n",
    "metrics_keras = model_metrics(all_labels_keras, all_preds_keras, all_probs_keras, agri_class_labels)\n",
    "\n",
    "# get the PyTorch model performance metrics\n",
    "metrics_pytorch = model_metrics(all_labels_pytorch, all_preds_pytorch, all_probs_pytorch, agri_class_labels)\n",
    "\n",
    "\n",
    "# Display the comparison of metrics\n",
    "print(\"{:<18} {:<15} {:<15}\".format('\\033[1m'+ 'Metric' + '\\033[0m',\n",
    "                                    'Keras Model', \n",
    "                                    'PyTorch Model'))\n",
    "\n",
    "mertics_list = ['Accuracy', 'Precision', 'Recall', 'F1 Score', 'ROC-AUC']\n",
    "\n",
    "for k in mertics_list:\n",
    "    print(\"{:<18} {:<15.4f} {:<15.4f}\".format('\\033[1m'+k+'\\033[0m',\n",
    "                                              metrics_keras[k],\n",
    "                                              metrics_pytorch[k]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5367255-39c2-473d-95bb-393e8baf6622",
   "metadata": {},
   "source": [
    "### Metric analysis\n",
    "\n",
    "The metrics for the pre-trained Keras and PyTorch models for evaluating the provided dataset are:\n",
    "\n",
    "- **Accuracy**\n",
    "    1. Keras: 0.9925\n",
    "    2. PyTorch: 0.9988\n",
    "    \n",
    "    ===> Both models achieve exceptional accuracy, but the **PyTorch model makes fewer mistakes**.\n",
    "\n",
    "- **Precision**\n",
    "    1. Keras: 1.0000\n",
    "    2. PyTorch: 0.9983\n",
    "\n",
    "    ===> The **Keras** model perfectly **avoids false positives**, whereas the PyTorch model is slightly less perfect but still excellent.\n",
    "\n",
    "- **Recall**\n",
    "    1. Keras: 0.9850\n",
    "    2. PyTorch: 0.9993\n",
    "    \n",
    "    ===> The **PyTorch** model is marginally better at **identifying all true positives**, capturing nearly all actual positive cases, while the Keras model misses a few.\n",
    "\n",
    "- **F1 Score**\n",
    "    1. PyTorch: 0.9988\n",
    "    2. Keras: 0.9924\n",
    "    \n",
    "    ===> The F1 score, which balances precision and recall, favors the **PyTorch** model thanks to its **stronger recall**.\n",
    "\n",
    "- **ROC-AUC**\n",
    "    1. Keras: 1.0000\n",
    "    2. PyTorch: 1.0000\n",
    "    \n",
    "    ===> Both models reach maximum possible **discrimination between classes**, indicating outstanding capability for binary classification.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a30912d-7ebe-4a8f-b5f1-d14fb1a5a12f",
   "metadata": {},
   "source": [
    "### **Model comparison: Key insights**\n",
    "\n",
    "\n",
    "**PyTorch model strengths**\n",
    "\n",
    " - Achieves the highest scores in accuracy, recall, and F1, indicating extremely robust overall performance and near-perfect classification of positive cases\n",
    "- ROC-AUC of 1.0 shows perfect class separability\n",
    "\n",
    "\n",
    "**Keras model strengths**\n",
    "\n",
    "- Displays almost perfect precision every positive prediction made is correct\n",
    "- Also achieves perfect ROC-AUC, indicating outstanding discrimination ability\n",
    "\n",
    "\n",
    "**Common strength**\n",
    "\n",
    "- Both models deliver flawless ROC-AUC, suggesting both are highly effective for this classification task\n",
    "\n",
    "\n",
    "**Recommendations**\n",
    "\n",
    "Based on the scores from the uploaded pre-trained models:\n",
    "\n",
    "- The PyTorch model is preferable for applications where missing any positive instances is costly (higher recall)\n",
    "- The Keras model is optimal for scenarios where making any false positive error is unacceptable (higher precision).\n",
    "\n",
    "\n",
    "**Next**\n",
    "\n",
    "- Analyze the confusion matrices to investigate the errors.\n",
    "- Monitor real-world performance, as even marginal differences can become important in high-impact applications. \n",
    "\n",
    "\n",
    "**Summary**\n",
    "\n",
    "Both models excel in all evaluated metrics and would be highly reliable in production. The PyTorch model demonstrates a modest edge in recall and F1 score, while the Keras model maximizes precision. The choice between models should ultimately reflect the specific requirements and risk tolerance of your use case.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb79672-3cdf-47b4-ae97-43d53965d480",
   "metadata": {},
   "source": [
    "## Save and download the notebook for **final project** submission and evaluation\n",
    "\n",
    "You will need to save and download the completed notebook for final project submission and evaluation. \n",
    "<br>For saving and downloading the completed notebook, please follow the steps given below:</br>\n",
    "\n",
    "<font size = 4>  \n",
    "\n",
    "1) **Complete** all the tasks and questions given in the notebook.\n",
    "\n",
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/nv4jHlPU5_R1q7ZJrZ69eg/DL0321EN-M1L1-Save-IPYNB-Screenshot-1.png\" style=\"width:600px; border:0px solid black;\">\n",
    "\n",
    "2) **Save** the notebook.</style>\n",
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/9-WPWD4mW1d-RV5Il5otTg/DL0321EN-M1L1-Save-IPYNB-Screenshot-2.png\" style=\"width:600px; border:0px solid black;\">\n",
    "\n",
    "3) Identify and right click on the **correct notebook file** in the left pane.</style>\n",
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/RUSRPw7NT6Sof94B7-9naQ/DL0321EN-M1L1-Save-IPYNB-Screenshot-3.png\" style=\"width:600px; border:0px solid black;\">\n",
    "\n",
    "4) Click on **Download**.</style>\n",
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/HHry4GT-vhLEcRi1T_LHGg/DL0321EN-M1L1-Save-IPYNB-Screenshot-4.png\" style=\"width:600px; border:0px solid black;\">\n",
    "\n",
    "5) Download and **Save** the Jupyter notebook file on your computer **for final submission**.</style>\n",
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/hhsJbxc6R-T8_pXQGjMjvg/DL0321EN-M1L1-Save-IPYNB-Screenshot-5.png\" style=\"width:600px; border:0px solid black;\">\n",
    "  </font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c4ff47-d95c-400f-ae87-56ca79b2ba98",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Congratulations! You've successfully evaluated and compared two deep learning models, one using Keras and the other using the PyTorch framework.\n",
    "\n",
    "You learnt about a comprehensive workflow for comparing Keras and PyTorch models on the same dataset and got hands-on experience on:\n",
    "- data preparation\n",
    "- model loading\n",
    "- predicting dataset\n",
    "- metric computation\n",
    "- ROC visualization\n",
    "- Model performance comparison\n",
    "\n",
    "Using these framework independent metrics, you now know how to evaluate different models for their performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca5057e-a8f6-478d-8639-fd70fee4f8eb",
   "metadata": {},
   "source": [
    "<h2>Author</h2>\n",
    "\n",
    "[Aman Aggarwal](https://www.linkedin.com/in/aggarwal-aman)\n",
    "\n",
    "Aman Aggarwal is a PhD working at the intersection of neuroscience, AI, and drug discovery. He specializes in quantitative microscopy and image processing.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e075dc2f-6ffa-45a6-b2d8-860217305244",
   "metadata": {},
   "source": [
    "<!--\n",
    "## Change Log\n",
    "\n",
    "|  Date (YYYY-MM-DD) |  Version | Changed By  |  Change Description |\n",
    "|---|---|---|---|\n",
    "| 2025-07-14  | 1.0  | Aman  |  Created the lab |\n",
    "\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917371aa-f1b6-469e-b57f-cbb963d3eef7",
   "metadata": {},
   "source": [
    "© Copyright IBM Corporation. All rights reserved.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "prev_pub_hash": "a4d52cb879d68ff11658550d0ab0119df62da062a1361bfc915addcf5e1b237d"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
