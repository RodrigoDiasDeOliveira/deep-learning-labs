{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "  <a href=\"https://cognitiveclass.ai/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0321ENSkillsNetwork951-2022-01-01\">\n",
    "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DL0321EN-SkillsNetwork/image/IDSN-logo.png\" width=\"400\">\n",
    "  </a>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=left><font size = 6>Lab: Land Classification: CNN-Transformer Integration evaluation </font></h1>\n",
    "    \n",
    "<h2 align=left><font size = 5>Vision Transformer (ViT) Model Evaluation </font></h2>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimated time: 90 minutes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introdution\n",
    "\n",
    "This notebook presents an end-to-end workflow for importing, testing, and evaluating two Vision Transformer (ViT) models developed in Keras and PyTorch, respectively. \n",
    "The self-attention mechanism in the ViTs allows these models to learn complex and broad spatial dependencies, providing improved performance on a variety of vision tasks compared to traditional convolutional neural networks. However, the CNNs are adept in learning the local features very well and can be trained using relatively smaller datasets and is generally much more efficient in utilizing the computational resources, as compared to ViTs. A CNN-ViT hybrid architecture gains from both CNN and ViT model strengths, by getting local features extracted using CNNs, while the transformer part of the hybrid architecture can determine the global dependencies.\n",
    "\n",
    "This lab focuses on model loading, prediction on sample data, and quantitative evaluation of the ViT models created using KEras and PyTorch. You'll explore the details of the framework-specific implementations, test the consistency of results, and gain practical experience comparing deep learning models across different Python ecosystems. \n",
    "\n",
    "Upon completion, you will have a thorough understanding of loading the CNN-ViT hybrid models testing workflow, key evaluation metrics, and how high-level architectural concepts translate into practical model evaluation using both Keras and PyTorch.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "- Import and initialize pre-trained CNN - Vision Transformer hybrid models from two deep learning frameworks (Keras/TensorFlow and PyTorch).\n",
    "- Prepare and preprocess sample image data for inference.\n",
    "- Perform model inference and obtain prediction results from both models.\n",
    "- Compute and compare core evaluation metrics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "1. [Dataset download, extraction and paths](#Dataset-download,-extraction-and-paths)\n",
    "2. [Pre-trained model download](#Pre-trained-model-download)\n",
    "3. [Package installation](#Package-installation)\n",
    "4. [Library imports and setup](#Library-imports-and-setup)\n",
    "5. [Fix random seed for reproducibility](#Fix-random-seed-for-reproducibility)\n",
    "6. [Defining PyTorch model architecture](#Defining-PyTorch-model-architecture)\n",
    "7. [Dataset path and hyperparameters](#Dataset-path-and-hyperparameters)\n",
    "8. [PyTorch Dataloader](#PyTorch-Dataloader)\n",
    "9. [--- PyTorch pre-trained ViT model loading ---](#----PyTorch-pre-trained-ViT-model-loading----)\n",
    "10. [PyTorch model inference metrics](#PyTorch-model-inference-metrics)\n",
    "11. [Keras model loading](#Keras-model-loading)\n",
    "12. [Keras pre-trained ViT model loading](#----Keras-pre-trained-ViT-model-loading----)\n",
    "13. [Define dataloader](#Define-dataloader)\n",
    "14. [Collecting metrics for Keras-based CNN-ViT hybrid model](#Collecting-metrics-for-Keras-based-CNN-ViT-hybrid-model)\n",
    "15. [Import the evaluation metrics](#Import-the-evaluation-metrics)\n",
    "16. [Keras metrics reporting](#Keras-metrics-reporting)\n",
    "17. [PyTorch metrics reporting](#PyTorch-metrics-reporting)\n",
    "18. [ROC curve plotting](#ROC-curve-plotting)\n",
    "19. [Comparing model performance](#Comparing-model-performance)\n",
    "20. [Summary and discussion](#Summary-and-discussion)\n",
    "21. [Conclusion](#Conclusion)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset download, extraction and paths\n",
    "We begin by downloading the dataset for evaluation of the models.\n",
    "Here, you declare:\n",
    "1. The dataset URL from where the dataset would be downloaded.\n",
    "2. The dataset downloading primary function, based on `skillsnetwork` library.\n",
    "3. The dataset fallback downloading function, based on regular `http` downloading functions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define root directory and download `url`\n",
    "\n",
    "First, you define the root directory where all the data would be downloaded and extracted.\n",
    "Here, the `dataset_url` is assigned a direct link to a tar archive hosted on IBM's cloud storage. This URL points to the satellite image dataset used for land classification tasks. Using a cloud-based URL ensures accessibility without local storage dependencies. This setup facilitates automated downloads later in the notebook. \n",
    "\n",
    "If dealing with large datasets, monitor download times and implement retry mechanisms for robustness. This variable is key for the subsequent download functions, linking external data to the local workflow.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \".\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_url = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/4Z1fwRR295-1O3PMQBH6Dg/images-dataSAT.tar\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data download\n",
    "We begin by downloading the dataset for evaluation of the models.\n",
    "Here, you declare:\n",
    "1. The dataset downloading primary function, based on `skillsnetwork` library.\n",
    "2. The dataset fallback downloading function, based on regular `http` downloading functions.\n",
    "3. Download the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write permissions available for downloading and extracting the dataset tar file\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbb12c4c34534dc69c14e57b9c1521e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading images-dataSAT.tar:   0%|          | 0/20243456 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9efeff85ddc24db98fb7ff78f290c075",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6003 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to '.'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import skillsnetwork\n",
    "\n",
    "def check_skillnetwork_extraction(extract_dir):\n",
    "    \"\"\"Check if the environment allows symlink creation for download/extraction.\"\"\"\n",
    "    symlink_test = os.path.join(extract_dir, \"symlink_test\")\n",
    "    if not os.path.exists(symlink_test):\n",
    "        os.symlink(os.path.join(os.sep, \"tmp\"), symlink_test)\n",
    "        print(\"Write permissions available for downloading and extracting the dataset tar file\")\n",
    "        os.unlink(symlink_test)\n",
    "\n",
    "async def download_tar_dataset(url, tar_path, extract_dir):\n",
    "    \"\"\"Download and extract dataset tar file asynchronously.\"\"\"\n",
    "    if not os.path.exists(tar_path):\n",
    "        try:\n",
    "            print(f\"Downloading from {url}...\")\n",
    "            import httpx\n",
    "            async with httpx.AsyncClient() as client:\n",
    "                response = await client.get(url, follow_redirects=True)\n",
    "                response.raise_for_status()\n",
    "                with open(tar_path, \"wb\") as f:\n",
    "                    f.write(response.content)\n",
    "            print(f\"Successfully downloaded '{tar_path}'.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Download error: {e}\")\n",
    "    else:\n",
    "        print(f\"Dataset tar file already exists at: {tar_path}\")\n",
    "    import tarfile\n",
    "    with tarfile.open(tar_path, 'r:*') as tar_ref:\n",
    "        tar_ref.extractall(path=extract_dir)\n",
    "        print(f\"Successfully extracted to '{extract_dir}'.\")\n",
    "\n",
    "try:\n",
    "    check_skillnetwork_extraction(data_dir)\n",
    "    await skillsnetwork.prepare(url=dataset_url, path=data_dir, overwrite=True)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    print(\"Primary download/extraction method failed.\")\n",
    "    print(\"Falling back to manual download and extraction...\")\n",
    "    import tarfile\n",
    "    import httpx\n",
    "    from pathlib import Path\n",
    "    file_name = Path(dataset_url).name\n",
    "    tar_path = os.path.join(data_dir, file_name)\n",
    "    await download_tar_dataset(dataset_url, tar_path, data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-trained model download \n",
    "\n",
    "Now, define an asynchronous function to download model files from given URLs, if they are not already present locally. \n",
    "You use `httpx` for asynchronous HTTP requests with error handling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def download_model(url, model_path):\n",
    "    if not os.path.exists(model_path):\n",
    "        try:\n",
    "            print(f\"Downloading from {url}...\")\n",
    "            import httpx\n",
    "            async with httpx.AsyncClient() as client:\n",
    "                response = await client.get(url, follow_redirects=True)\n",
    "                response.raise_for_status()\n",
    "                with open(model_path, \"wb\") as f:\n",
    "                    f.write(response.content)\n",
    "            print(f\"Successfully downloaded '{model_path}'.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Download error: {e}\")\n",
    "    else:\n",
    "        print(f\"Model file already downloaded at: {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model paths and download\n",
    "\n",
    "In the cell below, you define the file paths and URLs for the Keras and PyTorch models and download them using the `download_model` function defined above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model file already downloaded at: ./keras_cnn_vit_ai_capstone.keras\n",
      "Model file already downloaded at: ./pytorch_cnn_vit_ai_capstone_model_state_dict.pth\n"
     ]
    }
   ],
   "source": [
    "data_dir = \".\"\n",
    "\n",
    "keras_model_url = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/7uNMQhNyTA8qSSDGn5Cc7A/keras-cnn-vit-ai-capstone.keras\"\n",
    "keras_model_name = \"keras_cnn_vit_ai_capstone.keras\"\n",
    "keras_model_path = os.path.join(data_dir, keras_model_name)\n",
    "\n",
    "pytorch_state_dict_url = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/rFBrDlu1NNcAzir5Uww8eg/pytorch-cnn-vit-ai-capstone-model-state-dict.pth\"\n",
    "pytorch_state_dict_name = \"pytorch_cnn_vit_ai_capstone_model_state_dict.pth\"\n",
    "pytorch_state_dict_path = os.path.join(data_dir, pytorch_state_dict_name)\n",
    "\n",
    "await download_model(keras_model_url, keras_model_path)\n",
    "await download_model(pytorch_state_dict_url, pytorch_state_dict_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Package installation\n",
    "\n",
    "### Install PyTorch, SkLearn library and Python Packages for evaluation metrics\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 432 ms, sys: 138 ms, total: 570 ms\n",
      "Wall time: 2min 18s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%capture captured_output\n",
    "%pip install scikit-learn==1.7.0 tensorflow==2.19 numpy==1.26 matplotlib==3.9.2 skillsnetwork\n",
    "%pip install torch==2.8.0+cpu torchvision==0.23.0+cpu torchaudio==2.8.0+cpu \\\n",
    "    --index-url https://download.pytorch.org/whl/cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library imports and setup\n",
    "\n",
    "Import essential libraries for data manipulation, visualization, and suppresses warnings for cleaner notebook output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 488 ms, sys: 197 ms, total: 685 ms\n",
      "Wall time: 2.54 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "import time\n",
    "import httpx\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorFlow/Keras library imports\n",
    "\n",
    "These imports set the environment variables to reduce TensorFlow logging noise and imports Keras modules for model building and training. They detect GPU availability for device assignment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1764066596.406337     723 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1764066596.500999     723 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1764066596.642574     723 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1764066596.642614     723 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1764066596.642616     723 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1764066596.642618     723 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow 2.19.0  |  GPUs found: []\n",
      "CPU times: user 3.03 s, sys: 764 ms, total: 3.79 s\n",
      "Wall time: 18.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "gpu_list = tf.config.list_physical_devices('GPU')\n",
    "device = \"gpu\" if gpu_list != [] else \"cpu\"\n",
    "print(f\"TensorFlow {tf.__version__}  |  GPUs found: {tf.config.list_physical_devices('GPU')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch library imports\n",
    "\n",
    "Import core PyTorch modules for model building, optimization, data loading, and functional utilities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported libraries\n",
      "CPU times: user 2.64 s, sys: 916 ms, total: 3.56 s\n",
      "Wall time: 4.58 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "#import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import  random_split\n",
    "import torch.nn.functional as F\n",
    "\n",
    "print(\"Imported libraries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fix random seed for reproducibility\n",
    "\n",
    "Define `set_seed` to ensure reproducibility across Python, NumPy, TensorFlow, and PyTorch by seeding random generators and enabling deterministic cuDNN. \n",
    "\n",
    "Set `SEED` to 7331. This is useful for consistent results in stochastic processes like training or inference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global seed set to 7331 for Python, NumPy, PyTorch, and TensorFlow\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "# --- Seed global ---\n",
    "SEED = 7331\n",
    "\n",
    "def set_seed(seed=SEED):\n",
    "    # Python\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    \n",
    "    # NumPy\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # PyTorch\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "    # TensorFlow\n",
    "    tf.random.set_seed(seed)\n",
    "    \n",
    "    print(f\"Global seed set to {seed} for Python, NumPy, PyTorch, and TensorFlow\")\n",
    "\n",
    "# --- Aplicar seed ---\n",
    "set_seed(SEED)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model paths \n",
    "Check for the existence of the Keras and PyTorch model files. This ensures models are accessible before loading, preventing runtime errors. Use absolute paths for reliability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found the pre-trained Keras model:\n",
      "keras_cnn_vit_ai_capstone.keras --at------> ./keras_cnn_vit_ai_capstone.keras\n",
      "Found the pre-trained PyTorch model:\n",
      "pytorch_cnn_vit_ai_capstone_model_state_dict.pth --at------> ./pytorch_cnn_vit_ai_capstone_model_state_dict.pth\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(keras_model_path):\n",
    "    print(\"Unable to find the Keras model at give path. Please check...\")\n",
    "else:\n",
    "    print(f\"Found the pre-trained Keras model:\\n{keras_model_name} --at------> {keras_model_path}\")\n",
    "\n",
    "if not os.path.exists(pytorch_state_dict_path):\n",
    "    print(\"Unable to find the PyTorch model at give path. Please check...\")\n",
    "else:\n",
    "    print(f\"Found the pre-trained PyTorch model:\\n{pytorch_state_dict_name} --at------> {pytorch_state_dict_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining PyTorch model architecture\n",
    "In this cell, you will define the PyTorch CNN-ViT model architegcture, exactly as defined during the model training. You define the classes for CNN feature extractor, patch embedding, multi-head self-attention, transformer block, ViT, and CNN-ViT hybrid. \n",
    "\n",
    "The `evaluate` function computes loss and accuracy. This architecture combines CNN local features with ViT global attention. \n",
    "\n",
    "Parameters like depth and heads are configurable, and defined same as during training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# --- CNN Feature Extractor ---\n",
    "class CNNFeatureExtractor(nn.Module):\n",
    "    def __init__(self, in_channels=3):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.features(x)\n",
    "\n",
    "# --- Patch Embedding ---\n",
    "class PatchEmbed(nn.Module):\n",
    "    def __init__(self, in_chans=128, embed_dim=768, patch_size=16):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x)  # [B, embed_dim, H', W']\n",
    "        x = x.flatten(2).transpose(1, 2)  # [B, num_patches, embed_dim]\n",
    "        return x\n",
    "\n",
    "# --- Transformer Block ---\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim=768, num_heads=8, mlp_ratio=4.0):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.attn  = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim, int(embed_dim * mlp_ratio)),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(int(embed_dim * mlp_ratio), embed_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.norm1(x), self.norm1(x), self.norm1(x))[0]\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x\n",
    "\n",
    "# --- CNN-ViT Hybrid ---\n",
    "class CNN_ViT_Hybrid(nn.Module):\n",
    "    def __init__(self, num_classes=2, heads=8, depth=6, embed_dim=768, patch_size=16):\n",
    "        super().__init__()\n",
    "        self.cnn = CNNFeatureExtractor()\n",
    "        self.patch_embed = PatchEmbed(in_chans=128, embed_dim=embed_dim, patch_size=patch_size)\n",
    "        self.pos_embed = nn.Parameter(torch.randn(1, 1000, embed_dim))  # num_patches can be flexible\n",
    "        self.blocks = nn.ModuleList([TransformerBlock(embed_dim, heads) for _ in range(depth)])\n",
    "        self.head = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.cnn(x)\n",
    "        x = self.patch_embed(x)\n",
    "        x = x + self.pos_embed[:, :x.size(1), :]\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "        x = x[:, 0]  # take first token as CLS substitute\n",
    "        x = self.head(x)\n",
    "        return x\n",
    "\n",
    "# --- Função de avaliação ---\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss, running_corrects = 0.0, 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            running_corrects += (preds == labels).sum().item()\n",
    "    return running_loss / len(loader.dataset), running_corrects / len(loader.dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset path and hyperparameters\n",
    "Here, you set the dataset path and hyperparameters like image size, channels, batch size, classes, and labels. These are used for data loading and model configuration. Consistent dimensions ensure compatibility with model inputs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Define the dataset directory, dataloader and model hyperparameters. The dataloader and model hyperparameters should be same as used during training \n",
    "\n",
    "- Define the `dataset_path`\n",
    "\n",
    "- Define **hyperparameters common dataloader**\n",
    "    - `img_w`, `img_h = 64, 64`\n",
    "    - `batch_size = 128`\n",
    "    - `num_classes = 2`\n",
    "    - `agri_class_labels = [\"non-agri\", \"agri\"]`\n",
    "\n",
    "  \n",
    "- Define **hyperparameters for PyTorch CNN-Vit Hybrid model**. The values have to same as those used while training the hybrid model. \n",
    "    - `depth = 3`\n",
    "    - `attn_heads = 6`\n",
    "    - `embed_dim = 768`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset path: ./images_dataSAT\n",
      "Dataloader hyperparameters:\n",
      "  Image size: 64x64\n",
      "  Batch size: 128\n",
      "  Num classes: 2\n",
      "\n",
      "Hybrid CNN-ViT Model Hyperparameters:\n",
      "  Depth: 3\n",
      "  Attention heads: 6\n",
      "  Embedding dim: 768\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ---------------------------------------------------------\n",
    "# Dataset directory\n",
    "# ---------------------------------------------------------\n",
    "dataset_path = \"./images_dataSAT\"   # ajuste se necessário\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Dataloader hyperparameters (mesmos usados no treinamento)\n",
    "# ---------------------------------------------------------\n",
    "img_w, img_h = 64, 64\n",
    "batch_size = 128\n",
    "num_classes = 2\n",
    "agri_class_labels = [\"non-agri\", \"agri\"]\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Model hyperparameters (iguais aos usados no treino)\n",
    "# ---------------------------------------------------------\n",
    "depth = 3            # transformer block depth\n",
    "attn_heads = 6       # number of attention heads\n",
    "embed_dim = 768      # embedding dimension\n",
    "\n",
    "print(\"Dataset path:\", dataset_path)\n",
    "print(\"Dataloader hyperparameters:\")\n",
    "print(f\"  Image size: {img_w}x{img_h}\")\n",
    "print(f\"  Batch size: {batch_size}\")\n",
    "print(f\"  Num classes: {num_classes}\")\n",
    "print(\"\\nHybrid CNN-ViT Model Hyperparameters:\")\n",
    "print(f\"  Depth: {depth}\")\n",
    "print(f\"  Attention heads: {attn_heads}\")\n",
    "print(f\"  Embedding dim: {embed_dim}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Double-click **here** for the solution.\n",
    "<!--\n",
    "dataset_path = os.path.join(data_dir, \"images_dataSAT\")\n",
    "\n",
    "# hyperparameters common dataloader\n",
    "img_w, img_h = 64, 64\n",
    "batch_size = 128\n",
    "num_classes = 2\n",
    "agri_class_labels = [\"non-agri\", \"agri\"]\n",
    "\n",
    "# hyperparameters for PyTorch CNN-Vit Hybrid model\n",
    "depth = 3\n",
    "attn_heads = 6\n",
    "embed_dim = 768\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch Dataloader\n",
    "Defines transforms for resizing, tensor conversion, and normalization (ImageNet means/std). \n",
    "\n",
    "Loads dataset with ImageFolder and creates DataLoader for batching without shuffling for evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "class SingleFolderDataset(Dataset):\n",
    "    def __init__(self, folder_path, transform=None):\n",
    "        self.folder_path = folder_path\n",
    "        self.transform = transform\n",
    "        self.files = [f for f in os.listdir(folder_path) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.folder_path, self.files[idx])\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        label = 0  # você pode atribuir um label fixo ou outro esquema\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, label\n",
    "\n",
    "val_dataset = SingleFolderDataset('./images_dataSAT', transform=eval_transform)\n",
    "val_loader  = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Instantiate PyTorch model\n",
    "Check the availability of CUDA device and set the `device` parameter accordingly.\n",
    "\n",
    "Based on the `CNN_ViT_Hybrid` function, instantiate the PyTorch model and move the model to the available `device`\n",
    "\n",
    "In this cell, you will:\n",
    "1. instantiate the PyTorch CNN_ViT_Hybrid with the previously declared model parameters\n",
    "3. detect the device for model inference\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# --- Patch Embedding ---\n",
    "class PatchEmbed(nn.Module):\n",
    "    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Conv2d(\n",
    "            in_chans, embed_dim,\n",
    "            kernel_size=patch_size,\n",
    "            stride=patch_size\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x)                      # (B, embed_dim, H/P, W/P)\n",
    "        x = x.flatten(2).transpose(1, 2)      # (B, num_patches, embed_dim)\n",
    "        return x\n",
    "\n",
    "# --- Transformer Encoder Block ---\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, mlp_ratio=4.0):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.attn  = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)\n",
    "\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim, int(embed_dim * mlp_ratio)),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(int(embed_dim * mlp_ratio), embed_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.norm1(x), self.norm1(x), self.norm1(x))[0]\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x\n",
    "\n",
    "# --- CNN + ViT Hybrid Model ---\n",
    "class CNN_ViT_Hybrid(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        img_size=224,\n",
    "        patch_size=16,\n",
    "        in_chans=3,\n",
    "        embed_dim=256,\n",
    "        depth=6,\n",
    "        num_heads=8,\n",
    "        mlp_ratio=4.0,\n",
    "        num_classes=10\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # CNN backbone (simple example)\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(in_chans, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "\n",
    "        # Patch Embedding\n",
    "        self.patch_embed = PatchEmbed(\n",
    "            img_size=img_size // 4,\n",
    "            patch_size=patch_size,\n",
    "            in_chans=128,\n",
    "            embed_dim=embed_dim\n",
    "        )\n",
    "\n",
    "        # Positional Embeddings\n",
    "        self.pos_embed = nn.Parameter(torch.randn(1, 1000, embed_dim))\n",
    "\n",
    "        # Transformer\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(embed_dim, num_heads, mlp_ratio)\n",
    "            for _ in range(depth)\n",
    "        ])\n",
    "\n",
    "        # Classification Head\n",
    "        self.head = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)     # CNN\n",
    "        x = self.patch_embed(x)  # patches\n",
    "\n",
    "        x = x + self.pos_embed[:, :x.size(1), :]\n",
    "\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "\n",
    "        x = x[:, 0]              # CLS token substitute\n",
    "        x = self.head(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Double-click **here** for the solution.\n",
    "<!--\n",
    "# Check device availability\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Create model instance\n",
    "pytorch_model = CNN_ViT_Hybrid(num_classes=num_classes,\n",
    "                      heads=attn_heads,\n",
    "                      depth=depth,\n",
    "                      embed_dim=embed_dim).to(device)\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the PyTorch model on cpu\n"
     ]
    }
   ],
   "source": [
    "print(f\"Evaluating the PyTorch model on {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### --- PyTorch pre-trained ViT model loading ---\n",
    "In this cell, you will load the PyTorch model state dict with **`strict=False`** for flexibility.\n",
    "\n",
    "Thus, you prepare the model for inference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo pré-treinado carregado de ai_capstone_pytorch_state_dict.pth e pronto para inferência\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# --- Definir número de classes ---\n",
    "num_classes = 2  # ajuste conforme seu dataset\n",
    "\n",
    "# --- Caminho correto do state_dict pré-treinado ---\n",
    "pytorch_state_dict_path = \"ai_capstone_pytorch_state_dict.pth\"\n",
    "\n",
    "# --- Instanciar o modelo (mesmos hiperparâmetros usados no treino) ---\n",
    "model_infer = CNN_ViT_Hybrid(\n",
    "    num_classes=num_classes,\n",
    "    depth=12,\n",
    "    embed_dim=768,\n",
    "    patch_size=16\n",
    ").to(device)\n",
    "\n",
    "# --- Carregar pesos pré-treinados ---\n",
    "model_infer.load_state_dict(torch.load(pytorch_state_dict_path, map_location=device), strict=False)\n",
    "\n",
    "# --- Colocar modelo em modo avaliação ---\n",
    "model_infer.eval()\n",
    "\n",
    "print(f\"Modelo pré-treinado carregado de {pytorch_state_dict_path} e pronto para inferência\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch model inference metrics\n",
    "\n",
    "Now, you perform:\n",
    "1. inference on test_loader\n",
    "2. collecte prediction, labels, and probabilities (for class 1)\n",
    "3. Uses no_grad for efficiency and eval mode\n",
    "4. Use tqdm to show progress.\n",
    "5. Move the data to the training device (CPU/GPU).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "# -----------------------------\n",
    "# 1. Configuração do dispositivo\n",
    "# -----------------------------\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Usando device: {device}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Número de classes\n",
    "# -----------------------------\n",
    "num_classes = 2  # ajuste conforme seu dataset\n",
    "\n",
    "# -----------------------------\n",
    "# 3. Test dataset e DataLoader\n",
    "# -----------------------------\n",
    "eval_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                         [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "test_dir = './images_dataSAT'  # usando diretório principal sem subpasta test\n",
    "test_dataset = ImageFolder(root=test_dir, transform=eval_transform)\n",
    "\n",
    "# Ajuste para evitar erro de shared memory\n",
    "test_loader  = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=0)\n",
    "print(f\"Test dataset size: {len(test_dataset)} imagens\")\n",
    "\n",
    "# -----------------------------\n",
    "# 4. Instanciar o modelo\n",
    "# -----------------------------\n",
    "model_infer = CNN_ViT_Hybrid(\n",
    "    num_classes=num_classes,\n",
    "    depth=12,\n",
    "    embed_dim=768,\n",
    "    patch_size=16\n",
    ").to(device)\n",
    "\n",
    "# -----------------------------\n",
    "# 5. Carregar pesos pré-treinados\n",
    "# -----------------------------\n",
    "pytorch_state_dict_path = \"ai_capstone_pytorch_state_dict.pth\"\n",
    "model_infer.load_state_dict(torch.load(pytorch_state_dict_path, map_location=device), strict=False)\n",
    "\n",
    "# -----------------------------\n",
    "# 6. Preparar para inferência\n",
    "# -----------------------------\n",
    "model_infer.eval()\n",
    "\n",
    "# -----------------------------\n",
    "# 7. Inferência\n",
    "# -----------------------------\n",
    "all_labels = []\n",
    "all_preds  = []\n",
    "all_probs  = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in tqdm(test_loader, desc=\"Inferência\"):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model_infer(images)\n",
    "        probs   = F.softmax(outputs, dim=1)[:, 1]  # probabilidade da classe 1\n",
    "        preds   = torch.argmax(outputs, dim=1)\n",
    "\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_probs.extend(probs.cpu().numpy())\n",
    "\n",
    "print(\"Inferência concluída!\")\n",
    "print(f\"Total de imagens avaliadas: {len(all_labels)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras model loading\n",
    "\n",
    "To load the Keras based CNN-ViT hybrid model, you will\n",
    "\n",
    "- define **custom Keras layers** with serialization for model saving/loading for:\n",
    "    - `position embedding`\n",
    "    - `transformer block`\n",
    "\n",
    "This step is essential for reconstructing the ViT architecture in Keras.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positional embedding that Keras can track\n",
    "@tf.keras.utils.register_keras_serializable(package=\"Custom\")\n",
    "class AddPositionEmbedding(layers.Layer):\n",
    "    def __init__(self, num_patches, embed_dim, **kwargs):\n",
    "        super(AddPositionEmbedding, self).__init__(**kwargs)\n",
    "        self.num_patches = num_patches\n",
    "        self.embed_dim   = embed_dim\n",
    "        self.pos = self.add_weight(\n",
    "            name=\"pos_embedding\",\n",
    "            shape=(1, num_patches, embed_dim),\n",
    "            initializer=\"random_normal\",\n",
    "            trainable=True)\n",
    "\n",
    "    def call(self, tokens):\n",
    "        return tokens + self.pos\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"num_patches\": self.num_patches,\n",
    "            \"embed_dim\":   self.embed_dim,\n",
    "        })\n",
    "        return {**config}\n",
    "\n",
    "# One Transformer encoder block\n",
    "@tf.keras.utils.register_keras_serializable(package=\"Custom\")\n",
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads=8, mlp_dim=2048, dropout=0.1, **kwargs):\n",
    "        super(TransformerBlock, self).__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.mlp_dim   = mlp_dim\n",
    "        self.dropout   = dropout\n",
    "        self.mha  = layers.MultiHeadAttention(num_heads, key_dim=embed_dim)\n",
    "        self.norm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.mlp = tf.keras.Sequential([\n",
    "            layers.Dense(mlp_dim, activation=\"gelu\"),\n",
    "            layers.Dropout(dropout),\n",
    "            layers.Dense(embed_dim),\n",
    "            layers.Dropout(dropout)\n",
    "        ])\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.norm1(x + self.mha(x, x))\n",
    "        return self.norm2(x + self.mlp(x))\n",
    "\n",
    "    # ---- NEW ----\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"embed_dim\":  self.embed_dim,\n",
    "            \"num_heads\":  self.num_heads,\n",
    "            \"mlp_dim\":    self.mlp_dim,\n",
    "            \"dropout\":    self.dropout,\n",
    "        })\n",
    "        return {**config}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### --- Keras pre-trained ViT model loading ---\n",
    "\n",
    "Here, you will load the pre-trained Keras model using **`load_model`**, providing **custom objects** for deserialization of user-defined layers. This enables inference with the hybrid model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------- load CNN-ViT hybrid model ------------------\n",
    "keras_model = load_model(keras_model_name,\n",
    "                         custom_objects={\n",
    "                         \"AddPositionEmbedding\": AddPositionEmbedding,\n",
    "                         \"TransformerBlock\":     TransformerBlock\n",
    "                          })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define dataloader\n",
    "\n",
    "In this cell, you create an ImageDataGenerator for rescaling and a generator for flowing images from directory, matching PyTorch setup for consistent evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(rescale=1./255)\n",
    "prediction_generator = datagen.flow_from_directory(\n",
    "    dataset_path,\n",
    "    target_size=(img_w, img_h),\n",
    "    batch_size=batch_size,\n",
    "    class_mode=\"binary\",\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collecting metrics for Keras-based CNN-ViT hybrid model\n",
    "Now, run the inference of the Keras-based CNN-ViT hybrid model and collect the evaluation metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "all_probs_keras = keras_model.predict(prediction_generator, verbose=1)\n",
    "all_preds_keras = np.argmax(all_probs_keras, axis=1)\n",
    "all_labels_keras = prediction_generator.classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the evaluation metrics\n",
    "\n",
    "Here you define the functions to compute and print classification metrics including accuracy, precision, recall, F1 score, ROC-AUC, confusion matrix, and log loss. These functions support both Keras and PyTorch model outputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.metrics import (accuracy_score,\n",
    "                             precision_score,\n",
    "                             recall_score,\n",
    "                             f1_score,\n",
    "                             roc_curve, \n",
    "                             roc_auc_score,\n",
    "                             log_loss,\n",
    "                             classification_report,\n",
    "                             confusion_matrix,\n",
    "                             ConfusionMatrixDisplay,\n",
    "                            )\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "# define a function to get the metrics comprehensively\n",
    "def model_metrics(y_true, y_pred, y_prob, class_labels):\n",
    "    y_prob = np.array(y_prob)\n",
    "    if len(y_prob.shape)<2:\n",
    "        roc_score = roc_auc_score(y_true, y_prob)\n",
    "    elif len(y_prob.shape)==2:\n",
    "        roc_score = roc_auc_score(y_true, y_prob[:,1])\n",
    "    else:\n",
    "        roc_score = np.nan\n",
    "    metrics = {'Accuracy': accuracy_score(y_true, y_pred),\n",
    "               'Precision': precision_score(y_true, y_pred),\n",
    "               'Recall': recall_score(y_true, y_pred),\n",
    "               'Loss': log_loss(y_true, y_prob),\n",
    "               'F1 Score': f1_score(y_true, y_pred),\n",
    "               'ROC-AUC': roc_score,\n",
    "               'Confusion Matrix': confusion_matrix(y_true, y_pred),\n",
    "               'Classification Report': classification_report(y_true, y_pred, target_names=class_labels, digits=4),\n",
    "               \"Class labels\": class_labels\n",
    "              }\n",
    "    return metrics\n",
    "\n",
    "#function to print the metrics\n",
    "def print_metrics(y_true, y_pred, y_prob, class_labels, model_name):\n",
    "    metrics = model_metrics(y_true, y_pred, y_prob, class_labels)\n",
    "    \n",
    "    print(f\"Evaluation metrics for the \\033[1m{model_name}\\033[0m\")\n",
    "    print(f\"Accuracy: {'':<1}{metrics[\"Accuracy\"]:.4f}\")\n",
    "    if metrics[\"ROC-AUC\"] != np.nan:\n",
    "        print(f\"ROC-AUC: {'':<2}{metrics[\"ROC-AUC\"]:.4f}\")\n",
    "    print(f\"Loss: {'':<5}{metrics[\"Loss\"]:.4f}\\n\")\n",
    "    print(f\"Classification report:\\n\\n  {metrics[\"Classification Report\"]}\")\n",
    "    print(\"========= Confusion Matrix =========\")\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=metrics[\"Confusion Matrix\"],\n",
    "                                  display_labels=metrics[\"Class labels\"])\n",
    "\n",
    "    disp.plot()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras metrics reporting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Print the evaluation metrics using `print_metrics` function for the **Keras** ViT model with name `Keras CNN-Vit Hybrid Model`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Imprimir métricas do modelo Keras ViT usando a função print_metrics\n",
    "\n",
    "print_metrics(\n",
    "    y_true=y_true_keras,          # rótulos verdadeiros\n",
    "    y_pred=y_pred_keras,          # previsões do modelo\n",
    "    model_name=\"Keras CNN-ViT Hybrid Model\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Double-click **here** for the solution.\n",
    "<!--\n",
    "print_metrics(y_true = all_labels_keras,\n",
    "              y_pred = all_preds_keras,\n",
    "              y_prob = all_probs_keras,\n",
    "              class_labels = agri_class_labels,\n",
    "              model_name = \"Keras CNN-Vit Hybrid Model\"\n",
    "             )\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch metrics reporting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Print the evaluation metrics using `print_metrics` function for the **PyTorch** ViT model with model name `PyTorch CNN-Vit Hybrid Model`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imprimir métricas de avaliação do modelo PyTorch ViT\n",
    "print_metrics(\n",
    "    y_true=y_true_torch,               # rótulos verdadeiros\n",
    "    y_pred=y_pred_torch,               # previsões do modelo\n",
    "    model_name=\"PyTorch CNN-ViT Hybrid Model\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Double-click **here** for the solution.\n",
    "<!--\n",
    "print_metrics(y_true = all_labels_pytorch,\n",
    "              y_pred = all_preds_pytorch,\n",
    "              y_prob = np.array(all_probs_pytorch),\n",
    "              class_labels = agri_class_labels,\n",
    "              model_name = \"PyTorch CNN-Vit Hybrid Model\"\n",
    "             )\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC curve plotting\n",
    "\n",
    "First, define a function to plot ROC curves for binary or multi-class classification using scikit-learn's `roc_curve` and `roc_auc_score`. It handles both single-class and multi-class cases by binarizing labels if needed.\n",
    "\n",
    "Next, plot the ROC curves for both the models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_roc(y_true, y_prob, model_name):\n",
    "    n_classes = y_prob.shape[1] if y_prob.ndim > 1 else 1\n",
    "    if n_classes == 1:\n",
    "        fpr, tpr, _ = roc_curve(y_true, y_prob)\n",
    "        auc = roc_auc_score(y_true, y_prob)\n",
    "        plt.plot(fpr, tpr, label=f'{model_name} (AUC = {auc:.4f})')\n",
    "    else:\n",
    "        y_true_bin = label_binarize(y_true, classes=np.arange(n_classes))\n",
    "        for i in range(n_classes):\n",
    "            fpr, tpr, _ = roc_curve(y_true_bin[:, i], y_prob[:, i])\n",
    "            auc = roc_auc_score(y_true_bin[:, i], y_prob[:, i])\n",
    "            plt.plot(fpr, tpr, label=f'{model_name} class {i} (AUC = {auc:.4f})')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curve')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC curve plotting for both models\n",
    "\n",
    "Plot the ROC curves for both Keras and PyTorch models on the same figure for visual performance comparison.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc(np.array(all_labels_keras), np.array(all_probs_keras[:, 1]), \"Keras Model\")\n",
    "plt.show()\n",
    "plot_roc(np.array(all_labels_pytorch), np.array(all_probs_pytorch), \"PyTorch Model\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing model performance\n",
    "\n",
    "Now compare the performance of different models to understand which model would be the best performer for your land classification task.\n",
    "Computed metrics for both models are used to generate a comparison table for key scores. This facilitates quick performance assessment between frameworks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the Keras model performance metrics\n",
    "metrics_keras = model_metrics(all_labels_keras, all_preds_keras, all_probs_keras, agri_class_labels)\n",
    "\n",
    "# get the PyTorch model performance metrics\n",
    "metrics_pytorch = model_metrics(all_labels_pytorch, all_preds_pytorch, all_probs_pytorch, agri_class_labels)\n",
    "\n",
    "\n",
    "# Display the comparison of metrics\n",
    "print(\"{:<18} | {:<15} {:<15}\".format('\\033[1m'+ 'Metric' + '\\033[0m',\n",
    "                                    'Keras Model', \n",
    "                                    'PyTorch Model'))\n",
    "print((\"\".join([\"-\" for _ in range(43)])))\n",
    "metrics_list = ['Accuracy', 'Precision', 'Recall', 'F1 Score', 'ROC-AUC']\n",
    "\n",
    "for k in metrics_list:\n",
    "    print(\"{:<18} | {:<15.4f} {:<15.4f}\".format('\\033[1m'+k+'\\033[0m',\n",
    "                                              metrics_keras[k],\n",
    "                                              metrics_pytorch[k]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and discussion\n",
    "\n",
    "This notebook showcased a framework-agnostic workflow for importing, testing, and evaluating Vision Transformer models built in both Keras and PyTorch. By running the same input through each model, we examined the compatibility of results and gained practical experience handling architectural and data format variations.\n",
    "\n",
    "Key insights include the criticality of input format alignment, the subtle differences in model serialization/loading, and the framework-induced variations in prediction outputs. For a more robust evaluation, repeat this process with a labeled validation dataset, compute further metrics (precision, recall, F1-score), and systematically analyze speed and resource usage.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save and download the notebook for **final project** submission and evaluation\n",
    "\n",
    "You will need to save and download the completed notebook for final project submission and evaluation. \n",
    "<br>For saving and downloading the completed notebook, please follow the steps given below:</br>\n",
    "\n",
    "<font size = 4>  \n",
    "\n",
    "1) **Complete** all the tasks and questions given in the notebook.\n",
    "\n",
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/nv4jHlPU5_R1q7ZJrZ69eg/DL0321EN-M1L1-Save-IPYNB-Screenshot-1.png\" style=\"width:600px; border:0px solid black;\">\n",
    "\n",
    "2) **Save** the notebook.</style>\n",
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/9-WPWD4mW1d-RV5Il5otTg/DL0321EN-M1L1-Save-IPYNB-Screenshot-2.png\" style=\"width:600px; border:0px solid black;\">\n",
    "\n",
    "3) Identify and right click on the **correct notebook file** in the left pane.</style>\n",
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/RUSRPw7NT6Sof94B7-9naQ/DL0321EN-M1L1-Save-IPYNB-Screenshot-3.png\" style=\"width:600px; border:0px solid black;\">\n",
    "\n",
    "4) Click on **Download**.</style>\n",
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/HHry4GT-vhLEcRi1T_LHGg/DL0321EN-M1L1-Save-IPYNB-Screenshot-4.png\" style=\"width:600px; border:0px solid black;\">\n",
    "\n",
    "5) Download and **Save** the Jupyter notebook file on your computer **for final submission**.</style>\n",
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/hhsJbxc6R-T8_pXQGjMjvg/DL0321EN-M1L1-Save-IPYNB-Screenshot-5.png\" style=\"width:600px; border:0px solid black;\">\n",
    "  </font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "Congratulations! You have successfully completed this lab and now you have a good understanding about loading custom pre-trained models, for both Keras and PyTorch frameworks. Using these pre-trained models, you can now evaluate their performance and also infer unknown datasets.\n",
    "I hope you have learnt to apply the key concepts of Keras/Pytorch based classifiers, both traditional CNNs and more advanced and state-of-the-art, CNN-ViT hybrid models. Using this knowledge, now you should be able to tackle a variety of real world image classification problems. Good luck!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Author</h2>\n",
    "\n",
    "[Aman Aggarwal](https://www.linkedin.com/in/aggarwal-aman)\n",
    "\n",
    "Aman Aggarwal is a PhD working at the intersection of neuroscience, AI, and drug discovery. He specializes in quantitative microscopy and image processing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--\n",
    "## Change Log\n",
    "\n",
    "|  Date (YYYY-MM-DD) |  Version | Changed By  |  Change Description |\n",
    "|---|---|---|---|\n",
    "| 2025-07-28  | 1.0  | Aman  |  Created the lab |\n",
    "\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "© Copyright IBM Corporation. All rights reserved.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "prev_pub_hash": "0617c40a894fed25e60cecc592899ab7c410ed1cb7dbf852edb7281fb679f1af"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
