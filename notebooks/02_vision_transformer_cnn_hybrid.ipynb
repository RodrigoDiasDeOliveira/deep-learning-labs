{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6498861f-52ef-4ba3-bbbe-9259792a610f",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "  <a href=\"https://cognitiveclass.ai/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0321ENSkillsNetwork951-2022-01-01\">\n",
    "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DL0321EN-SkillsNetwork/image/IDSN-logo.png\" width=\"400\">\n",
    "  </a>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba100652-ab0d-44a0-a145-00b5a2b06ff4",
   "metadata": {},
   "source": [
    "<h1 align=left><font size = 6>Lab: Vision Transformers Using PyTorch </font></h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5c1104-b4b0-427a-befb-1b205b14486c",
   "metadata": {},
   "source": [
    "<h5></h5>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915562dc-e4a2-4da0-a4b5-3d4a605d2987",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this notebook, you will learn to build a PyTorch-based hybrid Convolutional Neural Network (CNN) and Vision Transformer (ViT) for image classification. \n",
    "You'll start by using CNN layers to extract detailed features, such as edges and textures, from images. Then, you'll see how those features are passed to a Vision Transformer, which looks at the global correlations in the entire image by looking at all locations at once. Then, you will train a hybrid CNN-ViT model, and by the end of this lab, you'll also know how to monitor its performance. This approach gives you practical experience with state-of-the-art techniques in computer vision!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7955effd",
   "metadata": {},
   "source": [
    "<h2>Objective</h2>\n",
    "\n",
    "This notebook demonstrates how to use a custom trained PyTorch CNN model to extract feature maps and use them with a Vision Transformer (ViT) architecture to create a CNN-ViT hybrid architecture.\n",
    "\n",
    "After completing this lab, you will be able to:\n",
    "<ul>\n",
    "    \n",
    "1. Load the custom trained PyTorch CNN model\n",
    "2. Extract feature maps from the PyTorch model\n",
    "3. Prepare tokens for the Vision Transformer\n",
    "4. Build the Vision Transformer encoder\n",
    "5. Train and evaluate the hybrid model\n",
    "\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "toc",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "- [Model paths and download](#Model-paths-and-download)\n",
    "- [Defining pre-trained CNN backbone](#Defining-pre-trained-CNN-backbone)\n",
    "- [Vision Transformers](#Vision-Transformers)\n",
    "    - [Patch embedding](#Patch-embedding)\n",
    "    - [Multi-head self-attention (MHSA) module](#Multi-head-self-attention-(MHSA)-module)\n",
    "    - [Transformer block](#Transformer-block)\n",
    "    - [Vision Transformer (ViT) model](#Vision-Transformer-(ViT)-model)\n",
    "- [CNN-ViT hybrid model](#CNN-ViT-hybrid-model)\n",
    "- [Model training](#Model-training)\n",
    "- [Model evaluation](#Model-evaluation)\n",
    "- [Data preparation and loading](#Data-preparation-and-loading)\n",
    "- [Model initialization and training loop](#Model-initialization-and-training-loop)\n",
    "- [Plotting training and validation accuracy and loss](#Plotting-training-and-validation-accuracy-and-loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f18f62d",
   "metadata": {},
   "source": [
    "## Data download and extraction\n",
    "Let's begin by downloading the dataset for evaluation of the models.\n",
    "Here, you declare:\n",
    "1. The dataset URL from where the dataset would be downloaded\n",
    "2. The dataset downloading primary function, based on the `skillsnetwork` library\n",
    "3. The dataset fallback downloading function, based on regular `http` downloading functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b77975cb-407e-4066-b3c7-4cc5e59ceb77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write permissions available for downloading and extracting the dataset tar file\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54aaab36327c4e4ca54f505563060561",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading images-dataSAT.tar:   0%|          | 0/20243456 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "011131b70afb4ca8bbedb2e26de8a270",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6003 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to '.'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import skillsnetwork\n",
    "\n",
    "data_dir = \".\"\n",
    "dataset_url = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/4Z1fwRR295-1O3PMQBH6Dg/images-dataSAT.tar\"\n",
    "\n",
    "\n",
    "def check_skillnetwork_extraction(extract_dir):\n",
    "    \"\"\"Check if the environment allows symlink creation for download/extraction.\"\"\"\n",
    "    symlink_test = os.path.join(extract_dir, \"symlink_test\")\n",
    "    if not os.path.exists(symlink_test):\n",
    "        os.symlink(os.path.join(os.sep, \"tmp\"), symlink_test)\n",
    "        print(\"Write permissions available for downloading and extracting the dataset tar file\")\n",
    "        os.unlink(symlink_test)\n",
    "\n",
    "async def download_tar_dataset(url, tar_path, extract_dir):\n",
    "    \"\"\"Download and extract dataset tar file asynchronously.\"\"\"\n",
    "    if not os.path.exists(tar_path):\n",
    "        try:\n",
    "            print(f\"Downloading from {url}...\")\n",
    "            import httpx\n",
    "            async with httpx.AsyncClient() as client:\n",
    "                response = await client.get(url, follow_redirects=True)\n",
    "                response.raise_for_status()\n",
    "                with open(tar_path, \"wb\") as f:\n",
    "                    f.write(response.content)\n",
    "            print(f\"Successfully downloaded '{tar_path}'.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Download error: {e}\")\n",
    "    else:\n",
    "        print(f\"Dataset tar file already exists at: {tar_path}\")\n",
    "    import tarfile\n",
    "    with tarfile.open(tar_path, 'r:*') as tar_ref:\n",
    "        tar_ref.extractall(path=extract_dir)\n",
    "        print(f\"Successfully extracted to '{extract_dir}'.\")\n",
    "\n",
    "try:\n",
    "    check_skillnetwork_extraction(data_dir)\n",
    "    await skillsnetwork.prepare(url=dataset_url, path=data_dir, overwrite=True)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    print(\"Primary download/extraction method failed.\")\n",
    "    print(\"Falling back to manual download and extraction...\")\n",
    "    import tarfile\n",
    "    import httpx\n",
    "    from pathlib import Path\n",
    "    file_name = Path(dataset_url).name\n",
    "    tar_path = os.path.join(data_dir, file_name)\n",
    "    await download_tar_dataset(dataset_url, tar_path, data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c1f18c",
   "metadata": {},
   "source": [
    "## Package installation\n",
    "\n",
    "Install PyTorch and python packages libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b79b410f-de2d-4603-b062-f770fc469d00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 110 ms, sys: 49.7 ms, total: 159 ms\n",
      "Wall time: 39.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%capture captured_output\n",
    "%pip install numpy==1.26 matplotlib==3.9.2 skillsnetwork\n",
    "%pip install torch==2.8.0+cpu torchvision==0.23.0+cpu torchaudio==2.8.0+cpu \\\n",
    "    --index-url https://download.pytorch.org/whl/cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134c4ade",
   "metadata": {},
   "source": [
    "## Library imports and setup\n",
    "\n",
    "Import essential libraries for data manipulation, visualization, and suppresses warnings for cleaner notebook output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd0fcdf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 555 ms, sys: 148 ms, total: 704 ms\n",
      "Wall time: 805 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "import time\n",
    "import httpx\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "def present_time():\n",
    "        return datetime.now().strftime('%Y%m%d_%H%M%S')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46c7a98",
   "metadata": {},
   "source": [
    "### PyTorch library imports\n",
    "\n",
    "Import core PyTorch modules for model building, optimization, data loading, and functional utilities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92b7cb66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported PyTorch libraries\n",
      "CPU times: user 1.86 s, sys: 318 ms, total: 2.18 s\n",
      "Wall time: 2.35 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torch.nn.functional as F\n",
    "print(\"Imported PyTorch libraries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc53fe7",
   "metadata": {},
   "source": [
    "## Model download helper\n",
    "\n",
    "Now, define an asynchronous function to download model files from given URLs, if they are not already present locally. \n",
    "You use `httpx` for asynchronous HTTP requests with error handling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e211b54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def download_model(url, model_path):\n",
    "    if not os.path.exists(model_path):\n",
    "        try:\n",
    "            print(f\"Downloading from {url}...\")\n",
    "            import httpx\n",
    "            async with httpx.AsyncClient() as client:\n",
    "                response = await client.get(url, follow_redirects=True)\n",
    "                response.raise_for_status()\n",
    "                with open(model_path, \"wb\") as f:\n",
    "                    f.write(response.content)\n",
    "            print(f\"Successfully downloaded '{model_path}'.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Download error: {e}\")\n",
    "    else:\n",
    "        print(f\"Model file already downloaded at: {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c644faa0-0939-4668-a380-4c1ecce6e0e8",
   "metadata": {},
   "source": [
    "## Lab layout\n",
    "- First, you download the pre-trained PyTorch based CNN model.\n",
    "- Then, you define the CNN backbone. This is same as defined in the training of the pre-trained model.\n",
    "- The CNN backbone reduces the input image’s spatial dimensions and expands its feature channels. You then convert this feature map into a sequence of tokens for the Vision Transformer (ViT).\n",
    "- These tokens are passed into a ViT module. The ViT is applied after the CNN so it can model global relationships and context between different regions in the original image, something CNNs alone cannot do as effectively.\n",
    "- You use a sequential hybrid architecture: the CNN performs local feature extraction, and the ViT, using those extracted features, performs global reasoning. This leverages the strengths of both models for improved accuracy and generalization.\n",
    "- Feature reshaping is used: the CNN feature map is flattened and fed into the transformer, and positional encoding might be added to preserve spatial information.\n",
    "- You train this hybrid model end-to-end, meaning both the CNN and ViT parameters are updated together to optimize classification performance.\n",
    "- Throughout the process, you are able to monitor both local (CNN) and global (ViT) attention across the image, resulting in a model that is more robust and effective than using either approach alone\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e94bd9",
   "metadata": {},
   "source": [
    "## Model paths and download\n",
    "\n",
    "In the cell below, you define the file paths and URLs for the Keras and PyTorch models and download them using the `download_model` function defined above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c8d1c145-57c5-4031-ac2b-d207dd229ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \".\"\n",
    "\n",
    "pytorch_state_dict_url = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/8J2QEyQqD8x9zjrlnv6N7g/ai-capstone-pytorch-best-model-20250713.pth\"\n",
    "pytorch_state_dict_name = \"ai_capstone_pytorch_best_model_state_dict_downloaded.pth\"\n",
    "pytorch_state_dict_path = os.path.join(data_dir, pytorch_state_dict_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c5edf8ee-dab8-4a70-afd1-0a6cb65e7992",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model file already downloaded at: ./ai_capstone_pytorch_best_model_state_dict_downloaded.pth\n"
     ]
    }
   ],
   "source": [
    "await download_model(pytorch_state_dict_url, pytorch_state_dict_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d484b21c-163f-444a-8acb-134a04347a32",
   "metadata": {},
   "source": [
    "## Ensuring repeatability in PyTorch\n",
    "\n",
    "To achieve reproducible results when you train a CNN in PyTorch, you must seed every random-number generator and configures cuDNN for deterministic kernels.\n",
    "* **Python & NumPy** – Many data-pipeline utilities (shuffling lists, image augmentations) rely on these random-number generators. Seeding them first removes one entire layer of randomness.\n",
    "* **PyTorch CPU / GPU** – `torch.manual_seed` covers every op executed on the CPU, while `torch.cuda.manual_seed_all` applies the same seed to each GPU stream so that multi-GPU jobs stay in sync.\n",
    "* **cuDNN flags** – By default cuDNN picks the fastest convolution algorithm, which can vary run-to-run. Setting `deterministic=True` forces repeatable kernels and turning `benchmark` *off* prevents the auto-tuner from replacing those kernels mid-training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "53c7de30-36e4-4dae-bd80-be7442bb4c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed: int = 42) -> None:\n",
    "    \"\"\"Seed Python, NumPy, and PyTorch (CPU & all GPUs) and\n",
    "    make cuDNN run in deterministic mode.\"\"\"\n",
    "    # ---- Python and NumPy -------------------------------------------\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # ---- PyTorch (CPU  &  GPU) --------------------------------------\n",
    "    torch.manual_seed(seed)            \n",
    "    torch.cuda.manual_seed_all(seed)   \n",
    "\n",
    "    # ---- cuDNN: force repeatable convolutions -----------------------\n",
    "    torch.backends.cudnn.deterministic = True \n",
    "    torch.backends.cudnn.benchmark     = False \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ff9c5ae1-9248-431f-a606-5778f2f5ca01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global seed set to 7331 - main process is now deterministic.\n"
     ]
    }
   ],
   "source": [
    "SEED = 7331\n",
    "set_seed(SEED)\n",
    "print(f\"Global seed set to {SEED} - main process is now deterministic.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cnn-doc",
   "metadata": {},
   "source": [
    "## Defining pre-trained CNN backbone\n",
    "\n",
    "In this cell, you will create and implement a **ConvNet** class. This class serves as the convolutional backbone for the hybrid CNN-ViT architecture. \n",
    "The design of this class will be **same as the training architecture**, with six progressive convolutional blocks.\n",
    "\n",
    "- **`forward_features()`**: Returns the raw convolutional feature map (B, 1024, H, W) for use by the **ViT component**\n",
    "\n",
    "- **Role in hybrid architecture**: In the hybrid model, this CNN serves as a **local feature extractor**, capturing low-level patterns, edges, and textures before passing the feature-rich representation to the Vision Transformer for global context modeling. The 1024-channel output provides a rich semantic representation that the ViT can process as a sequence of tokens.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2a120e6b-5689-4be7-9709-d7ebfa3cd3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    ''' \n",
    "    Class to define the architecture same as the imported pre-trained CNN model for extracting the` feature map\n",
    "    '''\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 5, padding=2), nn.ReLU(), nn.MaxPool2d(2), nn.BatchNorm2d(32),\n",
    "            nn.Conv2d(32, 64, 5, padding=2), nn.ReLU(), nn.MaxPool2d(2), nn.BatchNorm2d(64),\n",
    "            nn.Conv2d(64, 128, 5, padding=2), nn.ReLU(), nn.MaxPool2d(2), nn.BatchNorm2d(128),\n",
    "            nn.Conv2d(128, 256, 5, padding=2), nn.ReLU(), nn.MaxPool2d(2), nn.BatchNorm2d(256),\n",
    "            nn.Conv2d(256, 512, 5, padding=2), nn.ReLU(), nn.MaxPool2d(2), nn.BatchNorm2d(512),\n",
    "            nn.Conv2d(512, 1024, 5, padding=2), nn.ReLU(), nn.MaxPool2d(2), nn.BatchNorm2d(1024)\n",
    "        )\n",
    "\n",
    "    def forward_features(self, x):\n",
    "        return self.features(x)      # (B,1024,H,W)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6e6958-07b3-4037-9f91-d7ac6e4d2f81",
   "metadata": {},
   "source": [
    "# Vision Transformers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929cdcfe-8228-4e8a-851e-bbd79f5f18e5",
   "metadata": {},
   "source": [
    "### Patch embedding\n",
    " The **PatchEmbed** class implements the **interface** between the CNN feature extractor and the Vision Transformer, converting spatial feature maps into a sequence of tokens suitable for self-attention processing.\n",
    "\n",
    "Unlike traditional ViT implementations that divide raw images into fixed-size patches, this hybrid approach operates on CNN feature maps. The implementation uses a **1×1 convolution** as a learned linear projection to transform the 1024-dimensional CNN features into the transformer's embedding dimension (default 768).\n",
    "\n",
    "The transformation process involves three key steps:\n",
    "1. **Channel projection**: `nn.Conv2d(in_ch, embed_dim, kernel_size=1)` reduces or expands the channel dimension from 1024 to the preferred embedding size\n",
    "2. **Spatial flattening**: `.flatten(2)` collapses the height and width dimensions (H×W) into a single sequence dimension\n",
    "3. **Tensor reshaping**: `.transpose(1,2)` reorders dimensions from (B, D, L) to (B, L, D) where L=H×W represents the sequence length\n",
    "\n",
    "**Integration with hybrid architecture**\n",
    "\n",
    "In the context of the hybrid model, this patch embedding serves as the **bridge** between local CNN features and global transformer processing. Since the CNN has already extracted meaningful local patterns, the patch embedding focuses on format conversion rather than feature extraction. This design is more **efficient** than traditional ViT patch embedding since the CNN has already performed the heavy lifting of feature extraction from raw pixels.\n",
    "\n",
    "The **output tensor** (B, L, D) represents a batch of sequences where each sequence contains L tokens (corresponding to spatial locations in the feature map) with D-dimensional embeddings ready for transformer processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "01a517d5-b6fa-4fe5-a848-02cfc84106eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbed(nn.Module):\n",
    "    def __init__(self, input_channel=1024, embed_dim=768):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Conv2d(input_channel, embed_dim, kernel_size=1)  # 1×1 conv\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x).flatten(2).transpose(1, 2)  # (B,L,D)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mhsa-doc",
   "metadata": {},
   "source": [
    "## Multi-head self-attention (MHSA) module\n",
    "\n",
    "The **MHSA** class implements the self-attention mechanism that enables the Vision Transformer to model long-range dependencies and global context across all spatial locations in the feature map.\n",
    "\n",
    "The implementation follows the scaled dot-product attention formula: **Attention(Q,K,V) = softmax(QK^T/√d)V**, where Q, K, and V represent query, key, and value matrices, respectively. The scaling factor 1/sqrt(d) prevents the dot products from becoming too large, which would push the softmax function into regions with extremely small gradients.\n",
    "\n",
    "The multi-head mechanism splits the embedding dimension across multiple attention heads, allowing the model to attend to different types of relationships simultaneously. \n",
    "\n",
    "### Key implementation details:\n",
    "\n",
    "- **Unified QKV projection**: A single linear layer `nn.Linear(dim, dim*3)` generates Q, K, and V matrices efficiently, reducing memory overhead compared to separate projections\n",
    "- **Head reshaping**: The tensor is reshaped from (B, N, D) to (B, heads, N, d) where d = D/heads, enabling parallel processing across heads\n",
    "- **Attention computation**: Matrix multiplication `torch.matmul(q, k.transpose(-2, -1))` computes attention scores, followed by scaling and softmax normalization\n",
    "- **Dropout regularization**: Two dropout layers prevent overfitting - one on attention weights and one on the final output\n",
    "\n",
    "\n",
    "Unlike CNNs, which have limited receptive fields, self-attention allows every token to interact directly with every other token in a single operation. This enables the model to **capture long-range spatial dependencies** that might be missed by purely convolutional approaches. \n",
    "\n",
    "#### In the hybrid architecture, this global modeling complements the local feature extraction performed by the CNN backbone.\n",
    "\n",
    "The attention weights provide interpretability, showing which spatial locations the model focuses on when making predictions. This component is valuable for both performance and explainability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9f7114dc-5a8b-4aea-8509-e2dd13bb8f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MHSA(nn.Module):\n",
    "    def __init__(self, dim, heads=8, dropout=0.):\n",
    "        super().__init__()\n",
    "        self.heads = heads\n",
    "        self.scale = (dim // heads) ** -0.5\n",
    "        self.qkv = nn.Linear(dim, dim * 3)\n",
    "        self.attn_drop = nn.Dropout(dropout)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(dropout)\n",
    "    def forward(self, x):\n",
    "        B, N, D = x.shape\n",
    "        q, k, v = self.qkv(x).chunk(3, dim=-1)\n",
    "        q = q.reshape(B, N, self.heads, -1).transpose(1, 2)  # (B, heads, N, d)\n",
    "        k = k.reshape(B, N, self.heads, -1).transpose(1, 2)\n",
    "        v = v.reshape(B, N, self.heads, -1).transpose(1, 2)\n",
    "        attn = torch.matmul(q, k.transpose(-2, -1)) * self.scale\n",
    "        attn = self.attn_drop(attn.softmax(dim=-1))\n",
    "        x = torch.matmul(attn, v).transpose(1, 2).reshape(B, N, D)\n",
    "        return self.proj_drop(self.proj(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aebd0ac5-89ec-4445-9d6d-e3cc86dfbae1",
   "metadata": {},
   "source": [
    "## Transformer block\n",
    "This code defines a building block used in modern deep learning models, especially in Vision Transformers. The class is called TransformerBlock, and it is responsible for helping the model “pay attention” to the most important parts of its input and improve the final predictions.\n",
    "\n",
    "Let’s break down what happens inside:\n",
    "\n",
    "**Layer normalization (nn.LayerNorm):** This helps stabilize and speed up training by normalizing each row of the input data, which is useful before applying attention or a neural network layer.\n",
    "\n",
    "**Attention layer (MHSA):** This is the “Multi-Head Self Attention” block. It enables the model to look at all positions in the sequence (or image patches) at once, figuring out which ones are most important for each output. It’s like giving the model the power to focus on the key parts of an image or sentence.\n",
    "\n",
    "**MLP (nn.Sequential):** This is a **Multi-Layer Perceptron** or a mini neural network, made up of linear (fully connected) layers, a special activation (GELU), and dropout for regularization. This MLP has:\n",
    "- One linear layer that expands the input dimension by mlp_ratio (for example, 4× wider).\n",
    "- A GELU activation function (a nonlinear operation, similar to ReLU).\n",
    "- Dropout for regularization (helps prevent overfitting).\n",
    "- Another linear layer that shrinks the data back to the original dimension.\n",
    "- Another dropout layer.\n",
    "Here, MLP is designed to help the model learn better representations by combining and transforming the information after the attention step.\n",
    "\n",
    "**Skip/Residual connections (x + ...):** These connections support effective and stable training for deep neural networks by keeping pathways open for both forward information flow and backward gradient flow, making deep architectures such as transformers possible and practical. In transformers, skip connections are placed around both the attention and feedforward (MLP) sub-layers in each block. This stabilizes training of these very deep, stackable models, improves convergence speed, and lets them scale to larger datasets and more complex tasks.\n",
    "\n",
    "In summary, this block helps models understand relationships in their input data, making them more powerful for tasks such as image and language understanding!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "64f158e5-3c37-4682-8fe8-1db3d6813dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, dim, heads, mlp_ratio=4., dropout=0.):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.attn  = MHSA(dim, heads, dropout)\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        self.mlp   = nn.Sequential(\n",
    "                                    nn.Linear(dim, int(dim * mlp_ratio)),\n",
    "                                    nn.GELU(), nn.Dropout(dropout),\n",
    "                                    nn.Linear(int(dim * mlp_ratio), dim),\n",
    "                                    nn.Dropout(dropout))\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.norm1(x))\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vit-doc",
   "metadata": {},
   "source": [
    "## Vision Transformer (ViT) model\n",
    "\n",
    "The **ViT** class represents the complete Vision Transformer implementation, including patch embedding, positional encoding, transformer blocks, and a classification head for global context modeling.\n",
    "\n",
    "The implementation uses a learnable **Classification [CLS] token**:\n",
    "- **Initialization**: `nn.Parameter(torch.zeros(1, 1, embed_dim))` creates a learnable token initialized to zeros\n",
    "- **Expansion**: `self.cls.expand(B, -1, -1)` replicates the token across the batch dimension\n",
    "- **Prepending**: `torch.cat((cls, x), 1)` concatenates the CLS token to the beginning of the sequence\n",
    "- **Classification**: Only the CLS token representation is used for final classification\n",
    "\n",
    "This allows the CLS token to aggregate information from all spatial locations through self-attention, creating a global representation suitable for classification.\n",
    "\n",
    "`nn.Parameter(torch.randn(1, max_tokens, embed_dim))` creates a large positional embedding matrix\n",
    "and `self.pos[:, :L+1]` dynamically slices the positional embeddings to match the actual sequence length. Together, these create a **dynamic positional encoding** system for the hybrid architecture where the CNN feature map size can vary based on input image dimensions.\n",
    "\n",
    "\n",
    "**`depth`** defines the number of transformerBlocks to be used in the transformer encoder\n",
    "### Classification head design\n",
    "\n",
    "The final classification pipeline includes:\n",
    "1. **Final normalization**: `self.norm(x)` applies LayerNorm to the final transformer output\n",
    "2. **CLS token extraction**: `[:, 0]` selects only the CLS token representation\n",
    "3. **Linear classification**: `self.head` maps the CLS representation to class logits\n",
    "\n",
    "The model's ability to handle variable sequence lengths makes it robust to different input sizes and CNN architectures, providing flexibility in deployment scenarios.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4b1fd0bb-d07d-4b32-b441-2757d8b931b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT(nn.Module):\n",
    "    def __init__(self, in_ch=1024, num_classes=2,\n",
    "                 embed_dim=768, depth=6, heads=8,\n",
    "                 mlp_ratio=4., dropout=0.1, max_tokens=50):\n",
    "        super().__init__()\n",
    "        self.patch = PatchEmbed(in_ch, embed_dim)           # 1×1 conv\n",
    "        self.cls   = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        self.pos   = nn.Parameter(torch.randn(1, max_tokens, embed_dim))\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(embed_dim, heads, mlp_ratio, dropout)\n",
    "            for _ in range(depth)])\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.head = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):                          # x: (B,C,H,W)\n",
    "        x = self.patch(x)                          # (B,L,D)\n",
    "        B, L, _ = x.shape\n",
    "        cls = self.cls.expand(B, -1, -1)           # (B,1,D)\n",
    "        x = torch.cat((cls, x), 1)                 # (B,L+1,D)\n",
    "        x = x + self.pos[:, :L + 1]                # match seq-len\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "        return self.head(self.norm(x)[:, 0])       # CLS token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hybrid-doc",
   "metadata": {},
   "source": [
    "## CNN-ViT hybrid model\n",
    "\n",
    "The **CNN_ViT_Hybrid** class represents the hybrid architecture, integrating the CNN backbone with the Vision Transformer to create a unified model that leverages both local and global feature processing capabilities.\n",
    "In this hybrid model, the pre-trained CNN layers can be frozen while fine-tuning the Vi and allows for **efficient attention** to operate on semantically rich CNN features rather than raw pixels\n",
    "Thus, this hybrid approach combines the excellent capabilities of CNN to capture local patterns efficiently with ViTs' global context modeling, while keeping the level of complexity low.\n",
    "\n",
    "The forward pass implements a **two-stage forward pass pipeline**:\n",
    "1. **Local feature extraction**: `self.cnn.forward_features(x)` processes the input image through the CNN backbone, extracting hierarchical local features and reducing spatial resolution while increasing semantic depth.\n",
    "2. **Global context modeling**: `self.vit(features)` takes the CNN feature map and processes it through the Vision Transformer for global reasoning and classification.\n",
    "\n",
    "Despite the modular design, the entire hybrid model remains **fully differentiable**, enabling end-to-end training where gradients could flow from the classification loss back through both the ViT and CNN components. This allows the CNN to learn features that are optimally suited for the downstream transformer processing, creating a synergistic relationship between the two architectures.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dbf7d20d-9a55-4398-90a9-81e860f62838",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_ViT_Hybrid(nn.Module):\n",
    "    def __init__(self, num_classes=2, embed_dim=768, depth=6, heads=8):\n",
    "        super().__init__()\n",
    "        self.cnn = ConvNet(num_classes)            # load weights later\n",
    "        self.vit = ViT(num_classes=num_classes,\n",
    "                       embed_dim=embed_dim,\n",
    "                       depth=depth,\n",
    "                       heads=heads)\n",
    "    def forward(self, x):\n",
    "        return self.vit(self.cnn.forward_features(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "train-doc",
   "metadata": {},
   "source": [
    "# Model training\n",
    "\n",
    "The **train** function implements a comprehensive training loop for one epoch, handling forward propagation, loss computation, backpropagation, and metric tracking in a memory-efficient manner.\n",
    "\n",
    "**Training mode**\n",
    "The function begins with `model.train()`, which configures the model for training by enabling dropout layers and gradient computation\n",
    "\n",
    "**Batch processing pipeline**\n",
    "Each training iteration follows a standard deep learning pipeline:\n",
    "\n",
    "1. **Data transfer**: `x, y = x.to(device), y.to(device)` moves input data and labels to the appropriate device (CPU/GPU)\n",
    "2. **Gradient reset**: `optimizer.zero_grad()` clears gradients from the previous iteration to prevent accumulation\n",
    "3. **Forward pass**: `out = model(x)` computes predictions through the hybrid CNN-ViT architecture\n",
    "4. **Loss computation**: `loss = criterion(out, y)` calculates cross-entropy loss between predictions and ground truth\n",
    "5. **Backpropagation**: `loss.backward()` computes gradients via automatic differentiation\n",
    "6. **Parameter update**: `optimizer.step()` updates model parameters using the computed gradients\n",
    "\n",
    "**Metric accumulation**: The function tracks two key metrics:\n",
    "- **Weighted loss**: `loss.item() * x.size(0)` accumulates loss weighted by batch size for accurate averaging\n",
    "- **Correct predictions**: `(out.argmax(1) == y).sum().item()` counts correct predictions using argmax for multi-class classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "27c5540f-b131-4cfd-9d18-93d51b4dca23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    loss_sum, correct = 0, 0\n",
    "    for batch_idx, (x, y) in enumerate(tqdm(loader, desc=\"Training  \")):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(x)\n",
    "        loss = criterion(out, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_sum += loss.item() * x.size(0)\n",
    "        correct  += (out.argmax(1) == y).sum().item()\n",
    "    return loss_sum / len(loader.dataset), correct / len(loader.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eval-doc",
   "metadata": {},
   "source": [
    "## Model evaluation\n",
    "Here, you will evaluate the model, implementing inference without gradient computation to optimize memory usage and computational speed.\n",
    "\n",
    "Two fundamental differences between using the model for evaluation and testing versus training are:\n",
    "- **`torch.no_grad()`**:  Disables gradient computation and accelerate inference\n",
    "- **`model.eval()`**: Switches the model to evaluation mode, ensures deterministic outputs\n",
    "\n",
    "The **evaluation loop** mirrors the training loop structure but omits gradient-related operations.\n",
    "\n",
    "This function **integrates with the training pipeline**, providing regular validation checks that help monitor model progress, detect overfitting, and make informed decisions about training continuation, learning rate adjustments, and model selection. The consistent interface with the training function enables easy integration into automated training workflows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eaa42f92-0538-4490-a62f-8d1d37b64fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loader, criterion, device):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        loss_sum, correct = 0, 0\n",
    "        for batch_idx, (x, y) in enumerate(tqdm(loader, desc=\"Validation\")):\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            out = model(x)\n",
    "            loss = criterion(out, y)\n",
    "            loss_sum += loss.item() * x.size(0)\n",
    "            correct  += (out.argmax(1) == y).sum().item()\n",
    "    return loss_sum / len(loader.dataset), correct / len(loader.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202fafbd-1abd-4df8-a3a6-4d05b05f7307",
   "metadata": {},
   "source": [
    "## Data preparation and loading\n",
    "\n",
    "Here, you implement data preparation for the entire data pipeline, from raw image loading to batched tensor delivery.\n",
    "\n",
    "You define the key training hyperparameters:\n",
    "- **Image size (64×64)**: Chosen for computational efficiency while maintaining sufficient resolution for feature extraction\n",
    "- **Batch size (128)**: Balances memory usage with gradient stability and training speed\n",
    "- **learning rate (0.001)**: A conservative number for learning\n",
    "- **number of classes**: Total number of classes to be classified by the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c925c5c6-80f1-42f2-9212-f4c02bc50d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = os.path.join(data_dir, \"images_dataSAT\")\n",
    "\n",
    "img_size = 64\n",
    "batch_size = 32\n",
    "lr = 0.001\n",
    "num_cls  = 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b411ea27-22c9-404a-973b-e0c362e190f9",
   "metadata": {},
   "source": [
    "### Training data transformations\n",
    "The **training transform** pipeline implements several **augmentation techniques** including Random Rotation, Random Horizontal Flip, Random Affine with Shear and normalization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c719f2-ff7e-4e00-ad27-256a6d2cdc36",
   "metadata": {},
   "source": [
    "## Task: Create `train_transform` transforms for the training dataset\n",
    "- Use the following parameters:\n",
    "    - Size: `img_size x img_size`\n",
    "    - `RandomRotation`: 40\n",
    "    - `RandomHorizontalFlip`\n",
    "    - `RandomAffine(0, shear=0.2)`\n",
    "    - `Normalization` values: ([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "43b7aec3-13f2-4b33-8093-2edb676a86b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([transforms.Resize((img_size, img_size)),\n",
    "                                      transforms.RandomRotation(40),\n",
    "                                      transforms.RandomHorizontalFlip(),\n",
    "                                      transforms.RandomAffine(0, shear=0.2),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "                                     ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa3d4eb-f274-4fb7-be03-55235456b920",
   "metadata": {},
   "source": [
    "Double-click **here** for the solution.\n",
    "<!--\n",
    "train_transform = transforms.Compose([transforms.Resize((img_size, img_size)),\n",
    "                                      transforms.RandomRotation(40),\n",
    "                                      transforms.RandomHorizontalFlip(),\n",
    "                                      transforms.RandomAffine(0, shear=0.2),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "                                     ])\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c55c42-34cb-40ba-821a-f1d3f870ae0d",
   "metadata": {},
   "source": [
    "### Validation data transformations\n",
    "The **validation transform** is minimal for **deterministic preprocessing** to ensure reproducible validation results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e944b6e-9aa1-4413-ae6d-5943aa7abe2d",
   "metadata": {},
   "source": [
    "## Task: Create `val_transform` transforms for the validation dataset\n",
    "- Use the following parameters:\n",
    "    - Size: `img_size x img_size`\n",
    "    - `Normalization` values: ([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7f8d96f8-c895-4fc7-9818-384393dc238c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "# Definindo o tamanho das imagens\n",
    "img_size = 224  # você pode ajustar se tiver outra dimensão desejada\n",
    "\n",
    "# Transformações para o dataset de validação\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((img_size, img_size)),  # Redimensiona para img_size x img_size\n",
    "    transforms.ToTensor(),                     # Converte para tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],  # Normalização padrão ImageNet\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5870c47-4c92-4b81-b46d-3ce72eb45952",
   "metadata": {},
   "source": [
    "Double-click **here** for the solution.\n",
    "<!--\n",
    "val_transform = transforms.Compose([transforms.Resize((img_size, img_size)),\n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "                                    ])\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40beced-ab3f-4bda-876d-0a61b137382a",
   "metadata": {},
   "source": [
    "### The DataLoader\n",
    "\n",
    "The DataLoader setup is optimized for training:\n",
    "- **Shuffling**: Training data is shuffled to prevent batch-level patterns\n",
    "- **No Validation Shuffling**: Validation order is consistent for reproducible results\n",
    "- **`batch_size`**: Efficient tensor batching for GPU utilization\n",
    "\n",
    "You begin by splitting the dataset into `training` and `validation` data using `random_split` feature. Here, you define **80%** (0.8 fraction) of the total dataset for training and rest for validation.\n",
    "\n",
    "Next, you apply the `train_transform` to `train_dataset` and `val_transform` to `val_dataset` to make the dataset ready for DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e68dee90-ccc1-4933-b55e-cfa1ec1b2fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset = datasets.ImageFolder(dataset_path, transform=train_transform)\n",
    "\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "val_size = len(full_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
    "train_dataset.dataset.transform = train_transform\n",
    "val_dataset.dataset.transform = val_transform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96107c1-49a1-428b-b412-ff214cdfa2ac",
   "metadata": {},
   "source": [
    "## Task: Create the Dataloader `train_loader` and `val_loader` using `train_dataset` and `val_dataset`\n",
    "- Use the following parameters:\n",
    "    - `batch_size=batch_size`\n",
    "    - for `train_loader`: `shuffle=True`\n",
    "    - for `val_loader`: `shuffle=False`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1d72de3d-4f8e-445b-991d-9474ce10ffa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "import os\n",
    "\n",
    "# Tamanho das imagens e transformações\n",
    "img_size = 224\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((img_size, img_size)),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                         [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((img_size, img_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                         [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Diretórios dos datasets\n",
    "train_dir = './images_dataSAT'\n",
    "\n",
    "\n",
    "# Criar datasets\n",
    "train_dataset = ImageFolder(root=train_dir, transform=train_transform)\n",
    "val_dataset = ImageFolder(root=val_dir, transform=val_transform)\n",
    "\n",
    "# Criar DataLoaders\n",
    "batch_size = 32\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea67f23-a7f6-4a72-b6a7-5f8ac702561c",
   "metadata": {},
   "source": [
    "Double-click **here** for the solution.\n",
    "<!--\n",
    "train_loader = DataLoader(train_dataset, \n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True,\n",
    "                         )\n",
    "\n",
    "val_loader = DataLoader(val_dataset,\n",
    "                        batch_size=batch_size,\n",
    "                        shuffle=False,\n",
    "                       )\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loop-doc",
   "metadata": {},
   "source": [
    "## Model initialization and training loop\n",
    "\n",
    "This cell orchestrates the complete training pipeline, from model instantiation through iterative training and validation, implementing comprehensive monitoring and logging for effective model development.\n",
    "\n",
    "- **CUDA detection**: `torch.cuda.is_available()` checks for GPU availability\n",
    "\n",
    "The hybrid model is instantiated with carefully chosen hyperparameters:\n",
    "- **Number of classes**: `num_classes=2` configured for the specific dataset\n",
    "- **Default architecture**: Uses ViT configuration (768 embedding dim, 1  transformer layer, 1 heads)\n",
    "\n",
    "### Transfer learning integration\n",
    "\n",
    "The commented line demonstrates transfer learning capability:\n",
    "- **Pre-trained weights**: Option to load pre-trained CNN backbone weights\n",
    "- **Flexible loading**: `strict=False` allows partial weight loading\n",
    "\n",
    "**Optimizer**: The training uses the `adam` optimizer.\n",
    "\n",
    "The training loop collects the **training and validation metrics** to track training performance and monitor generalization of the model.\n",
    "\n",
    "### Training loop architecture\n",
    "\n",
    "Each epoch follows a structured pipeline:\n",
    "1. **Timing**: `time.time()` tracks epoch duration for performance monitoring\n",
    "2. **Training phase**: Calls the training function with appropriate parameters\n",
    "3. **Validation phase**: Evaluates model on validation set\n",
    "4. **Logging**: Comprehensive output showing all metrics and timing\n",
    "5. **Storage**: Appends metrics to tracking lists for later analysis\n",
    "\n",
    "In this training cell, you create a robust, monitored, and efficient training pipeline that provides comprehensive insights into model performance while maintaining computational efficiency and enabling easy debugging and optimization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1605bf28-929b-4eda-8da7-61ef0f8b0f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "from datetime import datetime\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# --- Função para mostrar hora atual ---\n",
    "def present_time():\n",
    "    return datetime.utcnow().strftime(\"%H:%M:%S\")\n",
    "\n",
    "# --- Definição da classe CNN_ViT_Hybrid ---\n",
    "class PatchEmbed(nn.Module):\n",
    "    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x)\n",
    "        x = x.flatten(2).transpose(1, 2)\n",
    "        return x\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, mlp_ratio=4.0):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.attn  = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim, int(embed_dim * mlp_ratio)),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(int(embed_dim * mlp_ratio), embed_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.norm1(x), self.norm1(x), self.norm1(x))[0]\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x\n",
    "\n",
    "class CNN_ViT_Hybrid(nn.Module):\n",
    "    def __init__(self, num_classes=2, heads=8, depth=6, embed_dim=768):\n",
    "        super().__init__()\n",
    "        # CNN backbone\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.patch_embed = PatchEmbed(img_size=224//4, patch_size=16, in_chans=128, embed_dim=embed_dim)\n",
    "        self.pos_embed = nn.Parameter(torch.randn(1, 1000, embed_dim))\n",
    "        self.blocks = nn.ModuleList([TransformerBlock(embed_dim, heads) for _ in range(depth)])\n",
    "        self.head = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.cnn(x)\n",
    "        x = self.patch_embed(x)\n",
    "        x = x + self.pos_embed[:, :x.size(1), :]\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "        x = x[:, 0]  # CLS token substitute\n",
    "        x = self.head(x)\n",
    "        return x\n",
    "\n",
    "# --- Configurações gerais ---\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Training the model on {device}\")\n",
    "\n",
    "epochs     = 5\n",
    "attn_heads = 6\n",
    "depth      = 3\n",
    "embed_dim  = 768\n",
    "batch_size = 16  # reduzir se memória for limitada\n",
    "num_classes = 2\n",
    "lr = 1e-3\n",
    "\n",
    "# --- Supondo que train_loader e val_loader já existam ---\n",
    "# Certifique-se de que num_workers=0 para evitar Bus error\n",
    "train_loader.num_workers = 0\n",
    "val_loader.num_workers   = 0\n",
    "\n",
    "# --- Instanciar modelo ---\n",
    "model = CNN_ViT_Hybrid(num_classes=num_classes, heads=attn_heads, depth=depth, embed_dim=embed_dim).to(device)\n",
    "\n",
    "# --- Funções de treinamento e avaliação ---\n",
    "def train(model, loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    running_loss, running_corrects = 0.0, 0\n",
    "    for images, labels in loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        running_corrects += (preds == labels).sum().item()\n",
    "    return running_loss/len(loader.dataset), running_corrects/len(loader.dataset)\n",
    "\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss, running_corrects = 0.0, 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            running_corrects += (preds == labels).sum().item()\n",
    "    return running_loss/len(loader.dataset), running_corrects/len(loader.dataset)\n",
    "\n",
    "# --- Criterion e optimizer ---\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# --- Treinamento ---\n",
    "best_loss = float('inf')\n",
    "tr_loss_all, te_loss_all = [], []\n",
    "tr_acc_all, te_acc_all = [], []\n",
    "training_time = []\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    start_time = time.time()\n",
    "    print(f\"\\nEpoch {epoch:02d}/{epochs:02d} started at {present_time()} (UTC)\")\n",
    "    tr_loss, tr_acc = train(model, train_loader, optimizer, criterion, device)\n",
    "    te_loss, te_acc = evaluate(model, val_loader, criterion, device)\n",
    "    print(f\"Epoch {epoch:02d} | train loss {tr_loss:.4f} acc {tr_acc:.4f} | val loss {te_loss:.4f} acc {te_acc:.4f} | in {time.time()-start_time:.02f}s\")\n",
    "    \n",
    "    tr_loss_all.append(tr_loss)\n",
    "    te_loss_all.append(te_loss)\n",
    "    tr_acc_all.append(tr_acc)\n",
    "    te_acc_all.append(te_acc)\n",
    "    training_time.append(time.time()-start_time)\n",
    "    \n",
    "    # Salvar o melhor modelo\n",
    "    if te_loss < best_loss:\n",
    "        print(f\"Current val loss ({te_loss:.04f}) lower than previous best ({best_loss:.04f}), saving model\")\n",
    "        best_loss = te_loss\n",
    "        torch.save(model.state_dict(), \"best_cnn_vit_model.pth\")\n",
    "\n",
    "print(\"Treinamento finalizado\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668583e1-9f69-4b2d-83dd-78d7ecbb5e27",
   "metadata": {},
   "source": [
    "### Hyperparameter cheatsheet (depth based)\n",
    "\n",
    "The Depth of the transformer blocks signify the number of transformer blocks stacked in the model. This is one of the most important features which differentiates a ViT from CNN.\n",
    "\n",
    "This table proides a basic overview on **how depth affects** other hyperparameters and model performance.\n",
    "\n",
    "|  **Depth** | **Attention heads** | **Dataset Size** |  **Performance** | **learning rate** | **Feature Complexity** | **Learning Focus** |\n",
    "|:---:|:---:|---|:---:|---|:---|:---:|\n",
    "| **3** | 6 | size < 1000 | Underfitting - too shallow |0.001 (Shallow: can handle higher learning rates) | Low-level features | Edges, textures, basic patterns |\n",
    "| **6** | 6 | size <1000 | Good for simple tasks |0.001 (Shallow: can handle higher learning rates) | Mid-level features | Shapes, object parts, spatial relationships |\n",
    "| **12** | 12 | 1000 < size < 10000 | Standard choice - good balance | 0.0005 (Medium: moderate learning rate) | High-level features | Objects, semantic concepts, global context |\n",
    "| **18** | 12 | 10000 < size < 100000 | High performance on complex tasks | 0.0003 (Deep: lower learning rate for stability) | High-level features | Objects, semantic concepts, global context |\n",
    "| **24** | 16 | 100000 < size | Diminishing returns, overfitting risk | 0.0001 (Very deep: very small learning rate) | High-level features | Objects, semantic concepts, global context |\n",
    "| **36** | 16 | 100000 < size | Likely overkill for most tasks | 0.0001 (Very deep: very small learning rate) | High-level features | Objects, semantic concepts, global context |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89dc7b27-134c-42f8-9034-decf80168e46",
   "metadata": {},
   "source": [
    "### Task: Design and train a CNN-ViT hybrid model `model_test` with the following hyperparameters:\n",
    "- `epochs=5`\n",
    "- `attn heads=12`\n",
    "- `transformer block depth = 12`\n",
    "- `embed_dim=768`\n",
    "\n",
    "Save the `accuracy` and `loss` metrics in\n",
    "- `tr_loss_all_test` for training loss\n",
    "- `te_loss_all_test` for validation/testing loss\n",
    "- `tr_acc_all_test` for training accuracy\n",
    "- `te_acc_all_test` for validation/testing accuracy\n",
    "\n",
    "Save the training times in `training_time_test`\n",
    "\n",
    "Save the best model as **`ai_capstone_pytorch_vit_model_test_state_dict.pth`**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c542f0ad-6d11-4afc-b21d-ffa6a7bf8330",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CNN-ViT Hybrid Model Test ---\n",
    "# Hyperparameters\n",
    "epochs_test     = 5\n",
    "attn_heads_test = 12\n",
    "depth_test      = 12\n",
    "embed_dim_test  = 768\n",
    "batch_size_test = 16  # ajuste conforme memória\n",
    "lr_test         = 1e-3\n",
    "\n",
    "# --- Instanciar modelo ---\n",
    "model_test = CNN_ViT_Hybrid(num_classes=num_classes, \n",
    "                            heads=attn_heads_test, \n",
    "                            depth=depth_test, \n",
    "                            embed_dim=embed_dim_test).to(device)\n",
    "\n",
    "# --- Criterion e optimizer ---\n",
    "criterion_test = nn.CrossEntropyLoss()\n",
    "optimizer_test = torch.optim.Adam(model_test.parameters(), lr=lr_test)\n",
    "\n",
    "# --- Inicializar listas de métricas ---\n",
    "tr_loss_all_test = []\n",
    "te_loss_all_test = []\n",
    "tr_acc_all_test  = []\n",
    "te_acc_all_test  = []\n",
    "training_time_test = []\n",
    "\n",
    "# --- Loop de treinamento ---\n",
    "best_loss_test = float('inf')\n",
    "\n",
    "for epoch in range(1, epochs_test+1):\n",
    "    start_time = time.time()\n",
    "    print(f\"\\nEpoch {epoch:02d}/{epochs_test:02d} started at {present_time()} (UTC)\")\n",
    "    \n",
    "    tr_loss, tr_acc = train(model_test, train_loader, optimizer_test, criterion_test, device)\n",
    "    te_loss, te_acc = evaluate(model_test, val_loader, criterion_test, device)\n",
    "    \n",
    "    print(f\"Epoch {epoch:02d} | train loss {tr_loss:.4f} acc {tr_acc:.4f} | val loss {te_loss:.4f} acc {te_acc:.4f} | in {time.time()-start_time:.02f}s\")\n",
    "    \n",
    "    tr_loss_all_test.append(tr_loss)\n",
    "    te_loss_all_test.append(te_loss)\n",
    "    tr_acc_all_test.append(tr_acc)\n",
    "    te_acc_all_test.append(te_acc)\n",
    "    training_time_test.append(time.time() - start_time)\n",
    "    \n",
    "    # Salvar o melhor modelo\n",
    "    if te_loss < best_loss_test:\n",
    "        print(f\"Current val loss ({te_loss:.04f}) lower than previous best ({best_loss_test:.04f}), saving model_test\")\n",
    "        best_loss_test = te_loss\n",
    "        torch.save(model_test.state_dict(), \"best_cnn_vit_model_test.pth\")\n",
    "\n",
    "print(\"Treinamento do model_test finalizado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251201c9-1224-4063-be10-0e8828654dcf",
   "metadata": {},
   "source": [
    "Double-click **here** for the solution.\n",
    "<!--\n",
    "## Please use the space below to write your answer\n",
    "\n",
    "device   = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Training the model on {device}\")\n",
    "\n",
    "epochs     = 5\n",
    "attn_heads = 12\n",
    "depth      = 12\n",
    "embed_dim  = 768\n",
    "\n",
    "print(f\"epochs:{epochs} | batch:{batch_size} | attn_heads:{attn_heads} | depth:{depth} | embed_dim:{embed_dim}\")\n",
    "\n",
    "model_dict_name = f\"ai_capstone_pytorch_vit_model_test_state_dict.pth\"\n",
    "\n",
    "model_test = CNN_ViT_Hybrid(num_classes=num_cls,\n",
    "                            heads=attn_heads,\n",
    "                            depth=depth,\n",
    "                            embed_dim=embed_dim\n",
    "                           ).to(device)\n",
    "\n",
    "# ------------------------------------------------------------------ #\n",
    "# loading pre-trained CNN weights\n",
    "model_test.cnn.load_state_dict(torch.load(pytorch_state_dict_path), strict=False)\n",
    "# ------------------------------------------------------------------ #\n",
    "\n",
    "criterion= nn.CrossEntropyLoss()\n",
    "optimizer= torch.optim.Adam(model_test.parameters(), lr=lr)\n",
    "\n",
    "best_loss = float('inf')\n",
    "tr_loss_all_test = []\n",
    "te_loss_all_test = []\n",
    "tr_acc_all_test = []\n",
    "te_acc_all_test = []\n",
    "training_time_test = []\n",
    "for epoch in range(1, epochs+1):\n",
    "    start_time = time.time()\n",
    "    print(f\"\\nEpoch {epoch:02d}/{epochs:02d} started at {present_time()} (UTC)\")\n",
    "    tr_loss,tr_acc = train(model_test, train_loader, optimizer, criterion, device)\n",
    "    te_loss,te_acc = evaluate(model_test, val_loader, criterion, device)\n",
    "    print(f\"Epoch {epoch:02d} | \"\n",
    "          f\"train loss {tr_loss:.4f} acc {tr_acc:.4f} | \"\n",
    "          f\"val loss {te_loss:.4f} acc {te_acc:.4f} |\"\n",
    "          f\" in  {time.time()-start_time:.02f}s\"\n",
    "        )\n",
    "    tr_loss_all_test.append(tr_loss)\n",
    "    te_loss_all_test.append(te_loss)\n",
    "    tr_acc_all_test.append(tr_acc)\n",
    "    te_acc_all_test.append(te_acc)\n",
    "    training_time_test.append(time.time() - start_time)\n",
    "\n",
    "    # Save the best model\n",
    "    avg_te_loss = te_loss\n",
    "    if avg_te_loss < best_loss:\n",
    "        print(f\"Current loss ({avg_te_loss:.04f}) lower than previous best loss ({ best_loss:.04f}), Saving current model state\")\n",
    "        best_loss = avg_te_loss\n",
    "        torch.save(model_test.state_dict(), model_dict_name)\n",
    "\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "plot-doc",
   "metadata": {},
   "source": [
    "## Plotting training and validation accuracy and loss\n",
    "\n",
    "This visualization cell creates comprehensive learning curves that provide crucial insights into model training dynamics, performance trends, and potential issues such as overfitting or underfitting.\n",
    "\n",
    "The implementation creates two separate plots for different aspects of training analysis:\n",
    "- **Accuracy plot**: Shows classification performance trends over epochs\n",
    "- **Loss plot**: Reveals optimization dynamics and convergence behavior\n",
    "\n",
    "\n",
    "These plots enable several important diagnostic assessments:\n",
    "- **Overfitting detection**: Widening gap between training and validation metrics\n",
    "- **Underfitting identification**: Both metrics plateau at suboptimal levels\n",
    "- **Training completion**: Convergence indicates when to stop training\n",
    "- **Hyperparameter evaluation**: Curves help assess learning rate, regularization effectiveness\n",
    "\n",
    "These learning curves serve as essential tools for understanding model behavior, diagnosing training issues, and making informed decisions about hyperparameter adjustments, training duration, and model architecture modifications.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a0050164-56da-44b4-99f5-1851f142aa10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig_w, fig_h = 6,4\n",
    "fig, axs = plt.subplots(figsize=(fig_w, fig_h ))\n",
    "\n",
    "# Plot Accuracy on the first subplot\n",
    "axs.plot(tr_acc_all, label='Training Accuracy')\n",
    "axs.plot(te_acc_all, label='Validation Accuracy')\n",
    "axs.set_title('Model Accuracy')\n",
    "axs.set_xlabel('Epochs')\n",
    "axs.set_ylabel('Accuracy')\n",
    "axs.legend()\n",
    "axs.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots( figsize=(fig_w, fig_h ))\n",
    "\n",
    "# Plot Loss on the second subplot\n",
    "axs.plot(tr_loss_all, label='Training Loss')\n",
    "axs.plot(te_loss_all, label='Validation Loss')\n",
    "axs.set_title('Model Loss')\n",
    "axs.set_xlabel('Epochs')\n",
    "axs.set_ylabel('Loss')\n",
    "axs.legend()\n",
    "axs.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3158bb89-889d-430b-90c2-d54c4e313171",
   "metadata": {},
   "source": [
    "## Task: Compare the performance of `model` with `model_test` by plotting the validation loss for `model` and `model_test` ViTs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3cf54d-9029-4094-81ad-e89e6b16ee62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Número de épocas\n",
    "epochs_model      = len(te_loss_all)\n",
    "epochs_model_test = len(te_loss_all_test)\n",
    "x_model = range(1, epochs_model + 1)\n",
    "x_test  = range(1, epochs_model_test + 1)\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(x_model, te_loss_all, marker='o', label='Validation Loss - model')\n",
    "plt.plot(x_test, te_loss_all_test, marker='s', label='Validation Loss - model_test')\n",
    "plt.title('Comparison of Validation Loss: model vs model_test')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation Loss')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038c595d-12ed-4e30-8ec9-bb615ca3c591",
   "metadata": {},
   "source": [
    "Double-click **here** for the solution.\n",
    "<!--\n",
    "\n",
    "fig, axs = plt.subplots( figsize=(fig_w, fig_h ))\n",
    "\n",
    "# Plot Loss on the second subplot\n",
    "axs.plot(te_loss_all, label='Validation Loss (model)')\n",
    "axs.plot(te_loss_all_test, label='Validation Loss (model_test)')\n",
    "axs.set_title('Model Loss')\n",
    "axs.set_xlabel('Epochs')\n",
    "axs.set_ylabel('Loss')\n",
    "axs.legend()\n",
    "axs.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c980db06-f25b-43f5-a20b-4f5e8363bf7c",
   "metadata": {},
   "source": [
    "## Task: Compare the training times of `model` with `model_test` by plotting the training time for each\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ef17b1-d965-4706-93d3-0b824fce7313",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Número de épocas\n",
    "epochs_model      = len(training_time)\n",
    "epochs_model_test = len(training_time_test)\n",
    "x_model = range(1, epochs_model + 1)\n",
    "x_test  = range(1, epochs_model_test + 1)\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(x_model, training_time, marker='o', label='Training Time - model')\n",
    "plt.plot(x_test, training_time_test, marker='s', label='Training Time - model_test')\n",
    "plt.title('Comparison of Training Time per Epoch')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Time (seconds)')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d3c82b-063f-4296-8686-10e33a6c633e",
   "metadata": {},
   "source": [
    "Double-click **here** for the solution.\n",
    "<!--\n",
    "\n",
    "fig, axs = plt.subplots( figsize=(fig_w, fig_h ))\n",
    "\n",
    "# Plot Loss on the second subplot\n",
    "axs.plot(training_time, label='Training time (model)')\n",
    "axs.plot(training_time_test, label='Training time (model_test)')\n",
    "axs.set_title('Training time')\n",
    "axs.set_xlabel('Epochs')\n",
    "axs.set_ylabel('Seconds')\n",
    "axs.legend()\n",
    "axs.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d9b69b-4e5e-45eb-9678-5fc2d7204189",
   "metadata": {},
   "source": [
   
